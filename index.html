<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />





<meta name="description" content="MIS, Gamification, Education">
<meta property="og:type" content="website">
<meta property="og:title" content="Jack Xia">
<meta property="og:url" content="http://blog.toob.net.cn/index.html">
<meta property="og:site_name" content="Jack Xia">
<meta property="og:description" content="MIS, Gamification, Education">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jack Xia">
<meta name="twitter:description" content="MIS, Gamification, Education">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blog.toob.net.cn/"/>





  <title>Jack Xia</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jack Xia</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">笔记</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-09-05-Study-ecomTech/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-09-05-Study-ecomTech/" itemprop="url">E-commerce technologies Lecture Notes</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-05T08:00:00+08:00">
                2020-09-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Introduction-Lecture-Notes"><a href="#Introduction-Lecture-Notes" class="headerlink" title="Introduction Lecture Notes"></a>Introduction Lecture Notes</h2><ol>
<li>key technologies<ol>
<li>Internet Infrastructure Technologies</li>
<li>Project Management and Development Life Cycle</li>
<li>Web Design, Implementation and Testing</li>
<li>Mobile and IoT compu8ng services and applications</li>
<li>Computational Intelligence and Machine Learning</li>
</ol>
</li>
<li>comprehensive framework<ol>
<li>Fundamentals of E-Commerce Security</li>
<li>Electronic Payment Systems</li>
<li>E-Financial Services</li>
<li>Building Smart Cities: an Information System Approach</li>
<li>E-Commerce and Data Science</li>
</ol>
</li>
<li>E-commerce-centric world for me<ol>
<li>Case study<ol>
<li>the purpose of Puma’s content management system</li>
<li>Alibaba Singles’ Day 2019</li>
</ol>
</li>
<li>Understanding E-commerce: Organizing Themes<ol>
<li>Technology - some of my projects<ol>
<li><a href="https://www.topbuy.com.au/" target="_blank" rel="noopener">Magento</a></li>
<li>B2C shopfiy ERP</li>
<li><a href="https://github.com/v-reap/EWP_OMS" target="_blank" rel="noopener">EWP_OMS</a></li>
<li><a href="https://github.com/v-reap/tbm" target="_blank" rel="noopener">Task System</a></li>
<li>Scratch, Scratchjr</li>
<li><a href="https://www.mycartoon.net.cn/" target="_blank" rel="noopener">A educational Game for coding</a></li>
<li>CRM</li>
</ol>
</li>
<li>Business<ol>
<li>CRM, ERP, OA and etc..</li>
<li>Business Process Management / Analysis</li>
<li>Financial management system</li>
<li>Business intelligence system</li>
<li>Knowledge Management System</li>
</ol>
</li>
<li>Society<ol>
<li>Organizational culture and training System</li>
<li>SNS Self-media on Facebook, Wechat and etc.</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>critical thinking and research skills<ol>
<li>Current E-Commerce trends<ol>
<li>social technologies, consumer-generated content, and services - negative impacts</li>
<li>Mobile first Design</li>
<li>5G, Broadband and wireless</li>
<li>self-media</li>
<li>E-Commerce and data science</li>
<li>AR, VR, XR and AI - Covid-19</li>
</ol>
</li>
<li>New Strategy for Traditional commerce:<ol>
<li>Passive consumer - consumer to consumer values</li>
<li>Sales-force driven - Boundary conditions for financial regulation and laws</li>
<li>Fixed prices - vary business models</li>
<li>Information asymmetry - <a href="https://www.sciencedirect.com/science/article/abs/pii/S0301421516301835" target="_blank" rel="noopener">Innovation and technology transfer through global value chains</a></li>
</ol>
</li>
<li>features of current E-Commerce technology<ol>
<li>Ubiquity - MIS, vary platform</li>
<li>Global reach - trends or not? China &amp; US</li>
<li>Universal standards - IT infrastructure(www, Visa, Cloud..), many Open Source project</li>
<li>Information richness - Google, Blog, CMS. How Managerial Behavior and Organization Design?</li>
<li>Interactivity - SNS, how about Information Security</li>
<li>Information density - problem in NLP, visual search?</li>
<li>Personalization/customization - tag consumers in CRM for analysis, products Recommendation, etc..</li>
<li>Social technology - changing consumer purchasing decision</li>
</ol>
</li>
<li>Types of E-Commerce<ol>
<li>which type for Intellectual property (e.g. Trademark, patent)?</li>
<li>B2B? B2G? G2B? B2G2B?</li>
</ol>
</li>
<li>Others<ol>
<li>Potential limitations on the growth of B2C E-Commerce</li>
<li>PredicOons/QuesOons/Challenges for the future</li>
</ol>
</li>
</ol>
</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-08-06-Work-Recruitment copy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-08-06-Work-Recruitment copy/" itemprop="url">电子商务与商标?知识产权?企业创新</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-06T08:00:00+08:00">
                2020-08-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h2><p>考虑申请Dissertation, 想将Dissertation和公司的IT工作结合起来做, 电子商务目前在知识产权领域的应用还处在非常初级的阶段, 这个领域比较新, 打算想想来整个Proposal试下, 先记录些资料:</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>项目为整合商标行政管理部门、商标代理服务机构、知识产权价值评估机构、互联网信息服务机构等方面资源，设立的从事商标设计、注册、评估、交易、质押、管理、保护等综合职能的互联网+知识产权平台服务.</p>
<p>本项目立足于从公司商标事业部的客观需求和主观预期出发，应用软件工程科学方法，利用成熟的开发工具，开发便捷、高效的商标管理和服务综合应用系统，融合商标设计、商标注册、商标交易、商标价值评估、商标质押等商业服务和商标合同备案、商标纠纷处置等商标行政管理的主要功能，提供一体化在线应用平台，将有条件为商标事业部的有效运作提供一定的技术保障，实现商标电子商务信息公开化、交易程序扁平化、工作流程规范化、系统操作网络化、业务数据集中化，并赋予商标事业部及第三方商标代理团队更高效的价值产出。该系统的投入使用，对推进公司商标现代化发展战略、提升公司商标知识产权服务效能、降低市场商标闲置率?、提高商标有效使用率?等方面具有较大意义.</p>
<p>大部分商标代理机构都设立了以机构介绍、业务宣传、商标展示等单向的信息宣传交流为主要功能的互联网网站, 包括:</p>
<ul>
<li>通达商标服务中心 <a href="http://www.tdtm.com.cn" target="_blank" rel="noopener">http://www.tdtm.com.cn</a></li>
<li>万慧达网 <a href="http://www.wanhuida.com" target="_blank" rel="noopener">http://www.wanhuida.com</a></li>
</ul>
<p>部分商标代理服务机构开发使用了部分具备商标在线设计、商标在线转让、商标注册代理等互动应用功能的平台网站，如:</p>
<ul>
<li>猪八戒知识产权 <a href="https://tg.ipr.zbj.com/trademark" target="_blank" rel="noopener">https://tg.ipr.zbj.com/trademark</a></li>
<li>超凡飙局 <a href="https://www.biaoju01.com" target="_blank" rel="noopener">https://www.biaoju01.com</a></li>
<li>中华商标超市 <a href="http://www.gbicom.cn" target="_blank" rel="noopener">http://www.gbicom.cn</a></li>
<li>中华商标买卖网 <a href="http://www.mark163.com" target="_blank" rel="noopener">http://www.mark163.com</a></li>
</ul>
<p>当下一线互联网公司也在其平台提供了相关企业服务:</p>
<ul>
<li>阿里云商标优选 <a href="https://tm.aliyun.com/trans" target="_blank" rel="noopener">https://tm.aliyun.com/trans</a></li>
<li>腾讯商标注册 <a href="https://tm.cloud.tencent.com/" target="_blank" rel="noopener">https://tm.cloud.tencent.com/</a></li>
<li>eBay VeRO项目 <a href="https://www.ebay.cn/newcms/Home/Rules/1" target="_blank" rel="noopener">https://www.ebay.cn/newcms/Home/Rules/1</a></li>
<li>Wish 跨境商标保护 <a href="https://merchant.wish.com/brand-partner" target="_blank" rel="noopener">https://merchant.wish.com/brand-partner</a></li>
</ul>
<h2 id="国内外研究发展现状"><a href="#国内外研究发展现状" class="headerlink" title="国内外研究发展现状"></a>国内外研究发展现状</h2><ol>
<li>运用软件工程方法对商标管理和服务综合应用系统进行分析和设计，应用成熟的软件开发工具，实现商标资讯传播、品牌展示、设计服务、注册代理、在线交易、价值评估、质押代理、合同备案、纠纷处理等基本功能。</li>
<li>基于现有的网络及通信手段，组合搭建与系统运行相适应的、必要的数据库服务器、Web服务器、备份服务器以及中间件产品，保障目标系统正常运行。</li>
<li>按照网络信息安全要求，建设必要的信息安全屏障机制，制定严格的系统实际</li>
</ol>
<h2 id="系统需求分析"><a href="#系统需求分析" class="headerlink" title="系统需求分析"></a>系统需求分析</h2><h3 id="业务需求分析"><a href="#业务需求分析" class="headerlink" title="业务需求分析"></a>业务需求分析</h3><h4 id="商标业务需求"><a href="#商标业务需求" class="headerlink" title="商标业务需求"></a>商标业务需求</h4><ol>
<li>商标代理管理<ol>
<li>代理合同管理</li>
<li>商标受理信息管理</li>
<li>商标基本信息管理</li>
</ol>
</li>
<li>商标代理信息检索<ol>
<li>代理合同信息检索<ol>
<li>近似查询: 图形+文字</li>
<li>综合查询: 商标号,申请人名称</li>
<li>状态查询: 商标号,注册号</li>
</ol>
</li>
<li>商标受理信息检索</li>
<li>客户信息检索</li>
<li>商标信息检索</li>
</ol>
</li>
<li>商标监测</li>
<li>系统管理<ol>
<li>用户管理</li>
<li>权限管理</li>
</ol>
</li>
</ol>
<h3 id="功能性需求分析"><a href="#功能性需求分析" class="headerlink" title="功能性需求分析"></a>功能性需求分析</h3><h4 id="基本功能需求"><a href="#基本功能需求" class="headerlink" title="基本功能需求"></a>基本功能需求</h4><h4 id="角色和权限管理需求"><a href="#角色和权限管理需求" class="headerlink" title="角色和权限管理需求"></a>角色和权限管理需求</h4><h3 id="非功能性需求分析"><a href="#非功能性需求分析" class="headerlink" title="非功能性需求分析"></a>非功能性需求分析</h3><h4 id="安全性需求"><a href="#安全性需求" class="headerlink" title="安全性需求"></a>安全性需求</h4><h4 id="性能需求分析"><a href="#性能需求分析" class="headerlink" title="性能需求分析"></a>性能需求分析</h4><h4 id="其他非功能性需求"><a href="#其他非功能性需求" class="headerlink" title="其他非功能性需求"></a>其他非功能性需求</h4><h2 id="技术架构"><a href="#技术架构" class="headerlink" title="技术架构"></a>技术架构</h2><p>系统总体架构<br>设计思路<br>系统分层结构<br>运行环境<br>系统功能模块设计<br>系统基本功能结构<br>系统业务模块设计<br>数据库设计<br>数据库需求分析<br>数据库概念结构设计<br>数据库逻辑结构设计<br>数据库物理结构设计<br>数据库设计图</p>
<h2 id="功能模块"><a href="#功能模块" class="headerlink" title="功能模块"></a>功能模块</h2><p>前台客户端主界面</p>
<p>用户注册模块</p>
<p>用户登录模块</p>
<p>用户中心模块</p>
<p>商标服务人员管理模块</p>
<h2 id="商标侵权"><a href="#商标侵权" class="headerlink" title="商标侵权"></a>商标侵权</h2><ol>
<li>诉讼地位<ol>
<li>原告（您告别人）<ol>
<li>丝类<ol>
<li>生丝<ol>
<li>白厂丝</li>
<li>注册类别</li>
</ol>
</li>
<li>双宫丝<ol>
<li>已注册并被认定为驰名商标</li>
<li>未注册但被认定为驰名商标</li>
</ol>
</li>
<li>绢丝</li>
<li>复合丝</li>
</ol>
</li>
<li>需提交的证据<ol>
<li>权利证据<ol>
<li>商标注册证、核准续展注册证明；</li>
<li>商标使用许可合同、在商标局备案的材料及商标注册证复印件或商标注册人的证明；</li>
<li>如果是注册商标财产权利的继承人起诉，还要有已经继承或正在继承的证据材料。</li>
</ol>
</li>
<li>侵权证据<ol>
<li>侵权产品及销售发票（如有）</li>
<li>对侵权行为进行公证的公证书等（如有）</li>
<li>对方突出使用企业字号的商品照片或宣传照片</li>
</ol>
</li>
<li>赔偿证据（权利人的实际损失或侵权人的违法所得）</li>
</ol>
</li>
<li>诉讼请求</li>
<li>您的个人信息<ol>
<li>您的身份信息</li>
<li>您的企业工商登记信息</li>
<li>您的联系方式</li>
<li>对方身份信息或企业名称（如有）</li>
<li>对方企业地址（如有）</li>
<li>对方商标名称（如有）</li>
</ol>
</li>
<li>您期望的解决方式<ol>
<li>非诉方式<ol>
<li>协商解决</li>
<li>发律师函</li>
<li>工商投诉<ol>
<li>侵权行为实施地<ol>
<li>生产地</li>
<li>销售地</li>
<li>运输地</li>
<li>仓储地</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>诉讼方式</li>
</ol>
</li>
</ol>
</li>
<li>被告（别人告您）<ol>
<li>收到原告的文件<ol>
<li>对方自行与您沟通（包括电话、邮件、书面等方式）</li>
<li>对方发律师函 - 函告内容、所述您的侵权事实、对方要求</li>
<li>对方已起诉至法院 - 有无收到法院传票 - 收到传票 - 上传传票</li>
</ol>
</li>
<li>您是否也进行了商标注册<ol>
<li>在先注册<ol>
<li>您注册商标的时间</li>
<li>您的商标与对方相同或相似</li>
<li>您的商品与对方相同或类似</li>
</ol>
</li>
<li>未注册但在先使用<ol>
<li>您有合理使用的理由</li>
<li>您的商标是否与对方相同或相似</li>
<li>您的商品是否与对方相同或类似</li>
<li>您是否规范使用商标</li>
</ol>
</li>
<li>您可能的答辩理由<ol>
<li>原告商标具有瑕疵，提出中止审理</li>
<li>被控侵权产品不是您生产的</li>
<li>您使用的商标与原告主张权利的商标不相同或不相近似</li>
<li>被控侵权商品与注册商标核准的商品不类似</li>
<li>您的行为属于正当使用或者未将标志作为商标使用</li>
</ol>
</li>
<li>您的需求及目标</li>
<li>您的个人信息<ol>
<li>您的身份信息</li>
<li>您的企业工商登记信息</li>
<li>您的联系方式</li>
<li>对方身份信息或企业名称（如有）</li>
<li>对方企业地址（如有）</li>
<li>对方商标名称（如有）</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>您的委托意向<ol>
<li>咨询且考虑委托</li>
<li>仅仅咨询不考虑委托</li>
</ol>
</li>
<li>所处的阶段 - 诉讼解决<ol>
<li>一审<ol>
<li>未立案</li>
<li>已立案 - 您是否已收到法院传票</li>
<li>已庭审 - 庭审情况</li>
<li>已判决且在上诉期内<ol>
<li>您是否不服一审判决</li>
<li>您是否有未提交的新证据</li>
</ol>
</li>
</ol>
</li>
<li>二审<ol>
<li>在上诉期内 - 您的上诉理由是什么</li>
<li>已上诉且受理<ol>
<li>是否接到开庭通知</li>
<li>您手中的新证据是否已经搜集</li>
</ol>
</li>
<li>已庭审 - 庭审过程情况及材料</li>
<li>已收到生效判决<ol>
<li>判决结果与您的预期是否一致</li>
<li>您认为判决是否有误</li>
<li>您是否有未提交的证据</li>
</ol>
</li>
</ol>
</li>
<li>再审<ol>
<li>当事人提出再审申请 - 您申请再审的理由</li>
</ol>
</li>
<li>诉讼时效<ol>
<li>侵犯注册商标专用权的诉讼时效为二年，自商标注册人或者利害关系人知道或者应当知道侵权行为之日起计算。</li>
<li>超过两年仍在连续侵权的，且仍在商标权利期限内的，法院可能判赔的数额是自权利人向法院提起诉讼之日前推二年</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="商标起名"><a href="#商标起名" class="headerlink" title="商标起名"></a>商标起名</h2><ol>
<li>起名悬赏<ol>
<li>发布起名任务<ol>
<li>商标起名</li>
<li>公司起名<ol>
<li>起名的需求信息<ol>
<li>标题</li>
<li>分类</li>
<li>商标是45个大类<ol>
<li>类别说明</li>
</ol>
</li>
<li>公司按行业分类</li>
</ol>
</li>
<li>要求<ol>
<li>系统设置的要求列表，用户多选<ol>
<li>字数、中、英文</li>
</ol>
</li>
</ol>
</li>
<li>具体要求<ol>
<li>富文本框</li>
</ol>
</li>
<li>截稿周期<ol>
<li>用户如果没有选择、或没有人投稿，由平台来进行关闭或者投稿关闭，悬赏金额平分给用户；没人投稿退款；</li>
<li>5天、7天、15天</li>
</ol>
</li>
<li>悬赏金额<ol>
<li>付款到平台</li>
<li>支付方式：暂定支付宝，微信支付</li>
<li>10,20,50元</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>任务广场<ol>
<li>投稿起名任务<ol>
<li>基本信息<ol>
<li>投稿名称（商标名、公司名）</li>
<li>是否查询<ol>
<li>商标、公司名称是否被注册</li>
</ol>
</li>
<li>起名说明</li>
</ol>
</li>
<li>投稿人资质认证后才可以申请</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>大师起名<ol>
<li>大师介绍<ol>
<li>大师个人介绍</li>
<li>案例介绍<ol>
<li>高 2988元</li>
<li>中 1988元</li>
<li>低套餐，金额不同 998元</li>
<li>郑青松国学起名网</li>
</ol>
</li>
</ol>
</li>
<li>选择大师<ol>
<li>发布起名需求<ol>
<li>行业，生辰八字，理想的名字几个字</li>
<li>选择套餐，付款到平台</li>
<li>大师的解释是否可以回复</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>智能起名<ol>
<li>一键起名<ol>
<li>根据所属行业生成商标名称</li>
</ol>
</li>
<li>衍生起名<ol>
<li>选择系统提供的随机商标关键字进行检索</li>
<li>用户自己输入的商标名称</li>
</ol>
</li>
<li>其它<ol>
<li>排序规则<ol>
<li>根据用户输入的文字进行模糊匹配，并根据匹配度进行排序</li>
<li>包含不仅限于文字匹配、拼音匹配、相近词匹配等</li>
</ol>
</li>
<li>引导<ol>
<li>引导用户去查询名字是否已经注册</li>
<li>引导用户去起名悬赏or大师起名</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>个人中心<ol>
<li>我的订单<ol>
<li>我的起名任务<ol>
<li>投稿列表<ol>
<li>选择&amp;关闭任务</li>
</ol>
</li>
<li>我投稿的任务<ol>
<li>任务详情</li>
</ol>
</li>
<li>大师起名订单</li>
</ol>
</li>
</ol>
</li>
<li>我的余额<ol>
<li>申请提现</li>
</ol>
</li>
<li>投稿人资质认证<ol>
<li>认证通过可以参与投稿</li>
<li>基本信息<ol>
<li>姓名</li>
<li>地区</li>
<li>擅长</li>
<li>证件</li>
<li>身份证</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>商标查询<ol>
<li>文字检索</li>
<li>图形检索</li>
<li>商标关注</li>
<li>商标注册<ol>
<li>风险分析</li>
</ol>
</li>
</ol>
</li>
<li>商标交易<ol>
<li>用户上传代售商标<ol>
<li>商标名称、商标类别、出售价格、上传商标证书、联系人、联系电话</li>
<li>是否为著名商标<ol>
<li>市知名，省著名，中国驰名</li>
<li>上传证书</li>
</ol>
</li>
<li>平台审核<ol>
<li>发布商品</li>
<li>发布竞拍高价商品</li>
</ol>
</li>
<li>合作协议</li>
</ol>
</li>
<li>商标出售<ol>
<li>普通商标B2C直接买卖</li>
<li>高价商标竞拍买卖</li>
</ol>
</li>
<li>交易成功列表</li>
</ol>
</li>
<li>商标注册<ol>
<li>子主题</li>
<li>前台展示，用户下单<ol>
<li>不注册支付下单</li>
<li>在线支付下单</li>
</ol>
</li>
<li>业务员跟进联系</li>
</ol>
</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-07-03-Work-community/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-07-03-Work-community/" itemprop="url">未命名</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-05T08:00:00+08:00">
                2020-07-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="企业社区-腾讯乐享"><a href="#企业社区-腾讯乐享" class="headerlink" title="企业社区 - 腾讯乐享"></a>企业社区 - 腾讯乐享</h2><p>企业微信里的腾讯乐享真不错, 免费还很强大, 超适合做企业文化和培训体系的建设, 整理了些想法, 等有空了搞搞.</p>
<h2 id="方案草稿"><a href="#方案草稿" class="headerlink" title="方案草稿"></a>方案草稿</h2><p>根据企业人才培养战略, 基于乐享一站式社区的系统规划, 制定必修课及考试, 并纳入考核, 考试通过后颁发相应证书</p>
<ol>
<li>需求收集<ol>
<li>通过投票调研需求和引导自测</li>
<li>通过乐问收集反馈</li>
<li>定义分类, 学习范围, 选修/必修</li>
</ol>
</li>
<li>培训体系应用<ol>
<li>通过学习地图制定培训计划 (闯关解锁课程)<ol>
<li>以入职培训为案例</li>
<li>以专业培训为案例</li>
<li>路径培养&amp;阶段学习案例(素质(能力,风格,动力)/人职匹配)</li>
</ol>
</li>
<li>通过K吧打造学习分享圈子</li>
<li>培训形式多样化, 直播培训后转课堂随时随地学习, 场景有<ol>
<li>技能培训</li>
<li>功能培训</li>
<li>销售培训</li>
<li>业务培训</li>
<li>流程培训</li>
</ol>
</li>
<li>学习效果评估<ol>
<li>考试评测(课前/课后)</li>
<li>课程相关/不相关调查评估(投票)</li>
<li>交流板块活跃度</li>
</ol>
</li>
<li>辅助功能<ol>
<li>多维度统计分析</li>
<li>消息提醒, 一键建群, 签到, 留言, 答疑</li>
<li>员工/讲师激励(投票评选,证书,积分)</li>
</ol>
</li>
</ol>
</li>
<li>运营与创新<ol>
<li>通过活动, 加快传播与创新</li>
<li>降低培训成本, 形成培训闭环</li>
<li>提高协作和交互, 量化评估培训质量</li>
<li>训战结合、导师制、社群学习、复盘等模式</li>
<li>基于员工定岗定级的能力模型应用对个人绩效的改进和组织使命和战略目标的贡献 [<a href="https://wenku.baidu.com/view/3c7f6b5d00020740be1e650e52ea551810a6c9e8.html#" target="_blank" rel="noopener">柯氏四级评估</a>=知识+技能+态度+信心+承诺]</li>
</ol>
</li>
</ol>
<ul>
<li>培训效果评估样例: 提高利润率<ol>
<li>领先指标<ol>
<li>每周销售报告</li>
<li>销售人员流动率</li>
</ol>
</li>
<li>关键行为<ol>
<li>经理们对销售员每周1次以上培训</li>
<li>经理们与销售员一起制定销售计划</li>
<li>经理们与销售员一起拜访客户 (至少每月一次)</li>
</ol>
</li>
<li>必须驱动手段<ol>
<li>经理们在每月第一天提交销售计划</li>
<li>每月共同拜访客户的名单</li>
<li>辅导所用的辅助工具</li>
</ol>
</li>
<li>第二级评估的学习目标<ol>
<li>经理们进行一个辅导谈话的角色扮演</li>
<li>经理们演练如何制定销售计划</li>
<li>经理们描述一个有效的客户拜访的构成要素</li>
</ol>
</li>
</ol>
</li>
</ul>
<p><img src="assets/培训/model.png" alt=""></p>
<ol>
<li>一级评估: 学员反应 (以学员为中心)<ol>
<li>我能将每一个学习目标与所学的内容联系起来</li>
<li>我从课程中得到的挑战是适度的,课程材料对我工作上的成功有极大的帮助</li>
<li>我能够很投入的参与培训</li>
<li>我能够积极的参与到课堂活动中去, 有充分的机会去练习我需要掌握的技能</li>
<li>在每次休息后, 我感到学习状态得到了恢复</li>
<li>我发现培训教室的布置和氛围有利于学习</li>
<li>引导学员关注培训的最终目标<ol>
<li>我相信所学内容值得应用到我的实际工作中去</li>
<li>我能够将所学应用到工作中去, 没有障碍</li>
<li>我认为我的努力能够给组织带来积极的结果</li>
</ol>
</li>
</ol>
</li>
<li>二级评估: 学习 (如何提高承诺? 例:我愿意尽力将所学应用到工作中去)<ol>
<li>我具备必要的知识和技能</li>
<li>我明确我要达到的期望</li>
<li>没有什么比这更重要</li>
<li>我知道如何获取必要的信息来源</li>
<li>我有着足够的来自公司的支持</li>
<li>公司要求我必须这么做</li>
<li>只要做了并达成最终目的,我就会获得奖励</li>
<li>其他</li>
</ol>
</li>
<li>三级评估: 行为改变<ol>
<li>确定关键行为 (例: 客户满意度提高10%)</li>
<li>建立必须的驱动力 (标杆, 鼓励, 业绩, 考核, 访谈)</li>
<li>讲授关键行为 (与学员相关的关键操作演练)</li>
<li>监控和衡量工作中关键操作/行为的绩效 (多方调研, 访谈, 培训知识在工作中的应用程度)</li>
<li>根据反馈进行调整 (成功案例法-罗伯特.布林克霍夫)</li>
</ol>
</li>
<li>四级评估: 业务结果, 举例:<ol>
<li>提高利润 ?%</li>
<li>降低成本 ?%</li>
<li>客户保有率 &amp; 市场份额 ?%</li>
<li>劳动生产率</li>
<li>跟高层管理人员确定培训期望值</li>
</ol>
</li>
</ol>
<h2 id="培训责任人"><a href="#培训责任人" class="headerlink" title="培训责任人"></a>培训责任人</h2><table>
<thead>
<tr>
<th style="text-align:center">分类</th>
<th style="text-align:center">管理员</th>
<th style="text-align:center">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">入职培训</td>
<td style="text-align:center">李X</td>
<td style="text-align:center">各地区个性化培训在部门吧进行</td>
</tr>
<tr>
<td style="text-align:center">XX事业部</td>
<td style="text-align:center">玲X</td>
</tr>
</tbody>
</table>
<h2 id="培训讲师"><a href="#培训讲师" class="headerlink" title="培训讲师"></a>培训讲师</h2><table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">培训组织方</th>
<th style="text-align:center">培训主题</th>
<th style="text-align:center">参训岗位/对象</th>
<th style="text-align:center">培训讲师</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">XX事业部</td>
<td style="text-align:center">方法类XX</td>
<td style="text-align:center">X咨询部</td>
<td style="text-align:center">张X</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-06-05-Work-Recruitment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-06-05-Work-Recruitment/" itemprop="url">大学排名备忘</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-05T08:00:00+08:00">
                2020-06-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="招聘相关"><a href="#招聘相关" class="headerlink" title="招聘相关"></a>招聘相关</h2><p>总是记不住这些排名.. 备忘下, 话说今年招人也太费劲了, 2个月面了100多人就招进来1个… 要疯了</p>
<table>
<thead>
<tr>
<th style="text-align:center">排名</th>
<th style="text-align:center">学校/校区</th>
<th style="text-align:center">批次分数线</th>
<th style="text-align:center">投档分</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">清华大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">685</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">北京大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">684</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">复旦大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">679</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">上海交通大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">678</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">复旦大学医学院</td>
<td style="text-align:center">502</td>
<td style="text-align:center">677</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">北京大学医学部</td>
<td style="text-align:center">502</td>
<td style="text-align:center">674</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">中国科学技术大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">667</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">南京大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">663</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">浙江大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">659</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">中国人民大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">658</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">同济大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">656</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">北京航空航天大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">656</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">浙江大学医学院</td>
<td style="text-align:center">502</td>
<td style="text-align:center">655</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">南开大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">646</td>
</tr>
<tr>
<td style="text-align:center">15</td>
<td style="text-align:center">哈尔滨工业大学（深圳）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">645</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">北京理工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">644</td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td style="text-align:center">西安交通大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">643</td>
</tr>
<tr>
<td style="text-align:center">18</td>
<td style="text-align:center">中国人民大学（苏州校区）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">640</td>
</tr>
<tr>
<td style="text-align:center">19</td>
<td style="text-align:center">华中科技大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">638</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">武汉大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">635</td>
</tr>
<tr>
<td style="text-align:center">21</td>
<td style="text-align:center">国防科技大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">634</td>
</tr>
<tr>
<td style="text-align:center">21</td>
<td style="text-align:center">天津大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">634</td>
</tr>
<tr>
<td style="text-align:center">23</td>
<td style="text-align:center">东南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">633</td>
</tr>
<tr>
<td style="text-align:center">23</td>
<td style="text-align:center">哈尔滨工业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">633</td>
</tr>
<tr>
<td style="text-align:center">25</td>
<td style="text-align:center">电子科技大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">632</td>
</tr>
<tr>
<td style="text-align:center">26</td>
<td style="text-align:center">厦门大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">631</td>
</tr>
<tr>
<td style="text-align:center">27</td>
<td style="text-align:center">中山大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">630</td>
</tr>
<tr>
<td style="text-align:center">27</td>
<td style="text-align:center">北京邮电大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">630</td>
</tr>
<tr>
<td style="text-align:center">29</td>
<td style="text-align:center">华南理工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">629</td>
</tr>
<tr>
<td style="text-align:center">29</td>
<td style="text-align:center">华东师范大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">629</td>
</tr>
<tr>
<td style="text-align:center">31</td>
<td style="text-align:center">电子科技大学（沙河校区）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">627</td>
</tr>
<tr>
<td style="text-align:center">32</td>
<td style="text-align:center">中央财经大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">625</td>
</tr>
<tr>
<td style="text-align:center">32</td>
<td style="text-align:center">中国政法大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">625</td>
</tr>
<tr>
<td style="text-align:center">34</td>
<td style="text-align:center">四川大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">624</td>
</tr>
<tr>
<td style="text-align:center">35</td>
<td style="text-align:center">中南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">623</td>
</tr>
<tr>
<td style="text-align:center">36</td>
<td style="text-align:center">西北工业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">622</td>
</tr>
<tr>
<td style="text-align:center">37</td>
<td style="text-align:center">重庆大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">621</td>
</tr>
<tr>
<td style="text-align:center">38</td>
<td style="text-align:center">山东大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">618</td>
</tr>
<tr>
<td style="text-align:center">38</td>
<td style="text-align:center">大连理工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">618</td>
</tr>
<tr>
<td style="text-align:center">38</td>
<td style="text-align:center">西安电子科技大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">618</td>
</tr>
<tr>
<td style="text-align:center">38</td>
<td style="text-align:center">哈尔滨工业大学（威海）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">618</td>
</tr>
<tr>
<td style="text-align:center">42</td>
<td style="text-align:center">北京交通大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">617</td>
</tr>
<tr>
<td style="text-align:center">43</td>
<td style="text-align:center">南京理工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">613</td>
</tr>
<tr>
<td style="text-align:center">44</td>
<td style="text-align:center">北京科技大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">612</td>
</tr>
<tr>
<td style="text-align:center">44</td>
<td style="text-align:center">中国传媒大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">612</td>
</tr>
<tr>
<td style="text-align:center">46</td>
<td style="text-align:center">湖南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">610</td>
</tr>
<tr>
<td style="text-align:center">46</td>
<td style="text-align:center">北京外国语大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">610</td>
</tr>
<tr>
<td style="text-align:center">46</td>
<td style="text-align:center">北京工业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">610</td>
</tr>
<tr>
<td style="text-align:center">49</td>
<td style="text-align:center">华东理工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">609</td>
</tr>
<tr>
<td style="text-align:center">49</td>
<td style="text-align:center">上海大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">609</td>
</tr>
<tr>
<td style="text-align:center">51</td>
<td style="text-align:center">中央民族大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">608</td>
</tr>
<tr>
<td style="text-align:center">51</td>
<td style="text-align:center">山东大学威海分校</td>
<td style="text-align:center">502</td>
<td style="text-align:center">608</td>
</tr>
<tr>
<td style="text-align:center">51</td>
<td style="text-align:center">北京邮电大学（宏福校区）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">608</td>
</tr>
<tr>
<td style="text-align:center">54</td>
<td style="text-align:center">空军军医大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">607</td>
</tr>
<tr>
<td style="text-align:center">54</td>
<td style="text-align:center">东北大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">607</td>
</tr>
<tr>
<td style="text-align:center">56</td>
<td style="text-align:center">华北电力大学（北京）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">606</td>
</tr>
<tr>
<td style="text-align:center">56</td>
<td style="text-align:center">中国海洋大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">606</td>
</tr>
<tr>
<td style="text-align:center">56</td>
<td style="text-align:center">苏州大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">606</td>
</tr>
<tr>
<td style="text-align:center">59</td>
<td style="text-align:center">吉林大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">604</td>
</tr>
<tr>
<td style="text-align:center">59</td>
<td style="text-align:center">兰州大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">604</td>
</tr>
<tr>
<td style="text-align:center">59</td>
<td style="text-align:center">武汉理工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">604</td>
</tr>
<tr>
<td style="text-align:center">62</td>
<td style="text-align:center">北京化工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">603</td>
</tr>
<tr>
<td style="text-align:center">62</td>
<td style="text-align:center">华中师范大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">603</td>
</tr>
<tr>
<td style="text-align:center">62</td>
<td style="text-align:center">上海外国语大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">603</td>
</tr>
<tr>
<td style="text-align:center">62</td>
<td style="text-align:center">暨南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">603</td>
</tr>
<tr>
<td style="text-align:center">66</td>
<td style="text-align:center">东华大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">602</td>
</tr>
<tr>
<td style="text-align:center">67</td>
<td style="text-align:center">河海大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">601</td>
</tr>
<tr>
<td style="text-align:center">68</td>
<td style="text-align:center">江南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">600</td>
</tr>
<tr>
<td style="text-align:center">68</td>
<td style="text-align:center">南京师范大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">600</td>
</tr>
<tr>
<td style="text-align:center">68</td>
<td style="text-align:center">西北大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">600</td>
</tr>
<tr>
<td style="text-align:center">71</td>
<td style="text-align:center">大连理工大学（盘锦校区）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">599</td>
</tr>
<tr>
<td style="text-align:center">71</td>
<td style="text-align:center">西南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">599</td>
</tr>
<tr>
<td style="text-align:center">73</td>
<td style="text-align:center">华北电力大学（保定）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">598</td>
</tr>
<tr>
<td style="text-align:center">74</td>
<td style="text-align:center">东北大学秦皇岛分校</td>
<td style="text-align:center">502</td>
<td style="text-align:center">596</td>
</tr>
<tr>
<td style="text-align:center">74</td>
<td style="text-align:center">西南交通大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">596</td>
</tr>
<tr>
<td style="text-align:center">74</td>
<td style="text-align:center">西南财经大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">596</td>
</tr>
<tr>
<td style="text-align:center">74</td>
<td style="text-align:center">哈尔滨工程大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">596</td>
</tr>
<tr>
<td style="text-align:center">74</td>
<td style="text-align:center">河北工业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">596</td>
</tr>
<tr>
<td style="text-align:center">74</td>
<td style="text-align:center">华南师范大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">596</td>
</tr>
<tr>
<td style="text-align:center">80</td>
<td style="text-align:center">中国地质大学（北京）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">595</td>
</tr>
<tr>
<td style="text-align:center">81</td>
<td style="text-align:center">中国地质大学（武汉）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">594</td>
</tr>
<tr>
<td style="text-align:center">81</td>
<td style="text-align:center">北京林业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">594</td>
</tr>
<tr>
<td style="text-align:center">83</td>
<td style="text-align:center">陕西师范大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">592</td>
</tr>
<tr>
<td style="text-align:center">83</td>
<td style="text-align:center">福州大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">592</td>
</tr>
<tr>
<td style="text-align:center">85</td>
<td style="text-align:center">中国石油大学（北京）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">591</td>
</tr>
<tr>
<td style="text-align:center">85</td>
<td style="text-align:center">天津医科大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">591</td>
</tr>
<tr>
<td style="text-align:center">87</td>
<td style="text-align:center">中国矿业大学（北京）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">590</td>
</tr>
<tr>
<td style="text-align:center">87</td>
<td style="text-align:center">北京中医药大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">590</td>
</tr>
<tr>
<td style="text-align:center">89</td>
<td style="text-align:center">中国农业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">589</td>
</tr>
<tr>
<td style="text-align:center">89</td>
<td style="text-align:center">安徽大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">589</td>
</tr>
<tr>
<td style="text-align:center">91</td>
<td style="text-align:center">中国药科大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">587</td>
</tr>
<tr>
<td style="text-align:center">91</td>
<td style="text-align:center">湖南师范大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">587</td>
</tr>
<tr>
<td style="text-align:center">93</td>
<td style="text-align:center">合肥工业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">586</td>
</tr>
<tr>
<td style="text-align:center">94</td>
<td style="text-align:center">郑州大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">585</td>
</tr>
<tr>
<td style="text-align:center">95</td>
<td style="text-align:center">中国石油大学（华东）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">584</td>
</tr>
<tr>
<td style="text-align:center">95</td>
<td style="text-align:center">云南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">584</td>
</tr>
<tr>
<td style="text-align:center">97</td>
<td style="text-align:center">长安大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">583</td>
</tr>
<tr>
<td style="text-align:center">98</td>
<td style="text-align:center">华中农业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">581</td>
</tr>
<tr>
<td style="text-align:center">98</td>
<td style="text-align:center">西北农林科技大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">581</td>
</tr>
<tr>
<td style="text-align:center">98</td>
<td style="text-align:center">东北师范大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">581</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">合肥工业大学（宣城校区）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">580</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">南京农业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">580</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">南京航空航天大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">580</td>
</tr>
<tr>
<td style="text-align:center">101</td>
<td style="text-align:center">辽宁大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">580</td>
</tr>
<tr>
<td style="text-align:center">105</td>
<td style="text-align:center">南昌大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">579</td>
</tr>
<tr>
<td style="text-align:center">105</td>
<td style="text-align:center">广西大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">579</td>
</tr>
<tr>
<td style="text-align:center">105</td>
<td style="text-align:center">太原理工大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">579</td>
</tr>
<tr>
<td style="text-align:center">108</td>
<td style="text-align:center">贵州大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">577</td>
</tr>
<tr>
<td style="text-align:center">109</td>
<td style="text-align:center">中国矿业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">575</td>
</tr>
<tr>
<td style="text-align:center">109</td>
<td style="text-align:center">上海财经大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">575</td>
</tr>
<tr>
<td style="text-align:center">111</td>
<td style="text-align:center">西南大学（荣昌校区）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">573</td>
</tr>
<tr>
<td style="text-align:center">112</td>
<td style="text-align:center">北京交通大学（威海校区）</td>
<td style="text-align:center">502</td>
<td style="text-align:center">572</td>
</tr>
<tr>
<td style="text-align:center">112</td>
<td style="text-align:center">海南大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">572</td>
</tr>
<tr>
<td style="text-align:center">114</td>
<td style="text-align:center">东北林业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">569</td>
</tr>
<tr>
<td style="text-align:center">115</td>
<td style="text-align:center">中南财经政法大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">563</td>
</tr>
<tr>
<td style="text-align:center">116</td>
<td style="text-align:center">内蒙古大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">562</td>
</tr>
<tr>
<td style="text-align:center">117</td>
<td style="text-align:center">东北农业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">561</td>
</tr>
<tr>
<td style="text-align:center">117</td>
<td style="text-align:center">四川农业大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">561</td>
</tr>
<tr>
<td style="text-align:center">119</td>
<td style="text-align:center">延边大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">557</td>
</tr>
<tr>
<td style="text-align:center">120</td>
<td style="text-align:center">青海大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">556</td>
</tr>
<tr>
<td style="text-align:center">121</td>
<td style="text-align:center">宁夏大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">555</td>
</tr>
<tr>
<td style="text-align:center">122</td>
<td style="text-align:center">新疆大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">547</td>
</tr>
<tr>
<td style="text-align:center">123</td>
<td style="text-align:center">中国石油大学（北京）克拉玛依校区</td>
<td style="text-align:center">502</td>
<td style="text-align:center">541</td>
</tr>
<tr>
<td style="text-align:center">124</td>
<td style="text-align:center">石河子大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">540</td>
</tr>
<tr>
<td style="text-align:center">125</td>
<td style="text-align:center">对外经济贸易大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">527</td>
</tr>
<tr>
<td style="text-align:center">126</td>
<td style="text-align:center">大连海事大学</td>
<td style="text-align:center">502</td>
<td style="text-align:center">515</td>
</tr>
<tr>
<td style="text-align:center">127</td>
<td style="text-align:center">西藏大学</td>
<td style="text-align:center">385</td>
<td style="text-align:center">489</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-05-05-Study-NQG-Paper-List/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-05-05-Study-NQG-Paper-List/" itemprop="url">Must-read papers for Neural Question Generation</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-05-05T08:00:00+08:00">
                2020-05-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>A summary of must-read papers for Neural Question Generation (NQG)</p>
<ul>
<li>Contributed by <strong><a href="http://www.liangmingpan.com" target="_blank" rel="noopener">Liangming Pan</a></strong> and <strong><a href="https://yuxixie.github.io/" target="_blank" rel="noopener">Yuxi Xie</a></strong>.</li>
</ul>
<h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a><a href="#content">Content</a></h2><ul>
<li><a href="#content">Content</a></li>
<li><a href="#survey-papers">Survey papers</a></li>
<li><a href="#models">Models</a><ul>
<li><a href="#basic-seq2seq-models">Basic Seq2Seq Models</a></li>
<li><a href="#encoding-answers">Encoding Answers</a></li>
<li><a href="#linguistic-features">Linguistic Features</a></li>
<li><a href="#question-specific-rewards">Question-specific Rewards</a></li>
<li><a href="#content-selection">Content Selection</a></li>
<li><a href="#question-type-modeling">Question Type Modeling</a></li>
<li><a href="#encode-wider-contexts">Encode Wider Contexts</a></li>
<li><a href="#other-directions">Other Directions</a></li>
</ul>
</li>
<li><a href="#applications">Applications</a><ul>
<li><a href="#difficulty-controllable-qg">Difficulty Controllable QG</a></li>
<li><a href="#conversational-qg">Conversational QG</a></li>
<li><a href="#asking-special-questions">Asking special questions</a></li>
<li><a href="#answer-unaware-qg">Answer-unaware QG</a></li>
<li><a href="#unanswerable-qg">Unanswerable QG</a></li>
<li><a href="#combining-qa-and-qg">Combining QA and QG</a></li>
<li><a href="#qg-from-knowledge-graphs">QG from knowledge graphs</a></li>
<li><a href="#visual-question-generation">Visual Question Generation</a></li>
<li><a href="#distractor-generation">Distractor Generation</a></li>
<li><a href="#cross-lingual-qg">Cross-lingual QG</a></li>
</ul>
</li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#resources">Resources</a></li>
</ul>
<h2 id="Survey-papers"><a href="#Survey-papers" class="headerlink" title="Survey papers"></a><a href="#content">Survey papers</a></h2><ol>
<li><p><strong>Recent Advances in Neural Question Generation.</strong> arxiv, 2018. <a href="https://arxiv.org/pdf/1905.08949.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Liangming Pan, Wenqiang Lei, Tat-Seng Chua, Min-Yen Kan.</em> </p>
</li>
</ol>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a><a href="#content">Models</a></h2><h3 id="Basic-Seq2Seq-Models"><a href="#Basic-Seq2Seq-Models" class="headerlink" title="Basic Seq2Seq Models"></a><a href="#basic-models">Basic Seq2Seq Models</a></h3><p>Basic Seq2Seq models with attention to generate questions. </p>
<ol>
<li><p><strong>Learning to ask: Neural question generation for reading comprehension.</strong> ACL, 2017. <a href="https://www.aclweb.org/anthology/P17-1123.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Xinya Du, Junru Shao, Claire Cardie.</em></p>
</li>
<li><p><strong>Neural question generation from text: A preliminary study.</strong> NLPCC, 2017. <a href="https://www.researchgate.net/profile/Franco_Scarselli/publication/4202380_A_new_model_for_earning_in_raph_domains/links/0c9605188cd580504f000000.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, Ming Zhou.</em></p>
</li>
<li><p><strong>Machine comprehension by text-to-text neural question generation.</strong> Rep4NLP@ACL, 2017. <a href="https://arxiv.org/pdf/1705.02012.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Xingdi Yuan, Tong Wang, Çaglar Gülçehre, Alessandro Sordoni, Philip Bachman, Saizheng Zhang, Sandeep Subramanian, Adam Trischler</em></p>
</li>
</ol>
<h3 id="Encoding-Answers"><a href="#Encoding-Answers" class="headerlink" title="Encoding Answers"></a><a href="#answer-encoding">Encoding Answers</a></h3><p>Applying various techniques to encode the answer information thus allowing for better quality answer-focused questions. </p>
<ol>
<li><p><strong>Answer-focused and Position-aware Neural Question Generation.</strong> EMNLP, 2018. <a href="https://www.aclweb.org/anthology/D18-1427" target="_blank" rel="noopener">paper</a></p>
<p><em>Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, Shi Wang</em></p>
</li>
<li><p><strong>Improving Neural Question Generation Using Answer Separation.</strong> AAAI, 2019. <a href="https://arxiv.org/pdf/1809.02393.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/yanghoonkim/NQG_ASs2s" target="_blank" rel="noopener">code</a></p>
<p> <em>Yanghoon Kim, Hwanhee Lee, Joongbo Shin, Kyomin Jung.</em></p>
</li>
<li><p><strong>Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring.</strong> AAAI, 2020. <a href="https://arxiv.org/pdf/1912.00879.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Xiyao Ma, Qile Zhu, Yanlin Zhou, Xiaolin Li, Dapeng Wu</em></p>
</li>
</ol>
<h3 id="Linguistic-Features"><a href="#Linguistic-Features" class="headerlink" title="Linguistic Features"></a><a href="#linguistic-features">Linguistic Features</a></h3><p>Improve QG by incorporating various linguistic features into the QG process. </p>
<ol>
<li><p><strong>Neural Generation of Diverse Questions using Answer Focus, Contextual and Linguistic Features.</strong> INLG, 2018. <a href="https://arxiv.org/pdf/1809.02637.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Vrindavan Harrison, Marilyn Walker</em></p>
</li>
<li><p><strong>Automatic Question Generation using Relative Pronouns and Adverbs.</strong> ACL, 2018. <a href="https://www.aclweb.org/anthology/P18-3022" target="_blank" rel="noopener">paper</a></p>
<p><em>Payal Khullar, Konigari Rachna, Mukul Hase, Manish Shrivastava</em> </p>
</li>
<li><p><strong>Learning to Generate Questions by Learning What not to Generate.</strong> WWW, 2019. <a href="https://arxiv.org/pdf/1902.10418.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/BangLiu/QG" target="_blank" rel="noopener">code</a></p>
<p> <em>Bang Liu, Mingjun Zhao, Di Niu, Kunfeng Lai, Yancheng He, Haojie Wei, Yu Xu.</em></p>
</li>
<li><p><strong>Improving Neural Question Generation using World Knowledge.</strong> arXiv, 2019. <a href="https://arxiv.org/pdf/1909.03716.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Deepak Gupta, Kaheer Suleman, Mahmoud Adada, Andrew McNamara, Justin Harris</em></p>
</li>
</ol>
<h3 id="Question-specific-Rewards"><a href="#Question-specific-Rewards" class="headerlink" title="Question-specific Rewards"></a><a href="#RL-rewards">Question-specific Rewards</a></h3><p>Improving the training via combining supervised and reinforcement learning to maximize question-specific rewards</p>
<ol>
<li><p><strong>Teaching Machines to Ask Questions.</strong> IJCAI, 2018. <a href="https://www.ijcai.org/proceedings/2018/0632.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Kaichun Yao, Libo Zhang, Tiejian Luo, Lili Tao, Yanjun Wu</em></p>
</li>
<li><p><strong>Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation</strong> arxiv, 2019. <a href="https://arxiv.org/pdf/1908.04942.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Yu Chen, Lingfei Wu, Mohammed J. Zaki</em></p>
</li>
<li><p><strong>Natural Question Generation with Reinforcement Learning Based Graph-to-Sequence Model</strong> NeurIPS Workshop, 2019. <a href="https://arxiv.org/pdf/1910.08832.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Yu Chen, Lingfei Wu, Mohammed J. Zaki</em></p>
</li>
<li><p><strong>Putting the Horse Before the Cart:A Generator-Evaluator Framework for Question Generation from Text</strong> CoNLL, 2019. <a href="https://arxiv.org/pdf/1808.04961.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Vishwajeet Kumar, Ganesh Ramakrishnan, Yuan-Fang Li</em></p>
</li>
<li><p><strong>Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering</strong> EMNLP, 2019. <a href="https://arxiv.org/pdf/1909.06356.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/ZhangShiyue/QGforQA" target="_blank" rel="noopener">code</a></p>
<p><em>Shiyue Zhang, Mohit Bansal</em></p>
</li>
</ol>
<h3 id="Content-Selection"><a href="#Content-Selection" class="headerlink" title="Content Selection"></a><a href="#content-selection">Content Selection</a></h3><p>Improve QG by considering how to select question-worthy contents (content selection) before asking a question. </p>
<ol>
<li><p><strong>Identifying Where to Focus in Reading Comprehension for Neural Question Generation.</strong> EMNLP, 2017. <a href="https://www.aclweb.org/anthology/D17-1219.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Xinya Du, Claire Cardie</em></p>
</li>
<li><p><strong>Answer-based Adversarial Training for Generating Clarification Questions.</strong> NAACL, 2019. <a href="https://arxiv.org/pdf/1904.02281.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/raosudha89/clarification_question_generation_pytorch" target="_blank" rel="noopener">code</a></p>
<p><em>Rao S, Daumé III H.</em></p>
</li>
<li><p><strong>Learning to Generate Questions by Learning What not to Generate.</strong> WWW, 2019. <a href="https://arxiv.org/pdf/1902.10418.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/BangLiu/QG" target="_blank" rel="noopener">code</a></p>
<p> <em>Bang Liu, Mingjun Zhao, Di Niu, Kunfeng Lai, Yancheng He, Haojie Wei, Yu Xu.</em></p>
</li>
<li><p><strong>Improving Question Generation With to the Point Context.</strong> EMNLP, 2019. <a href="https://arxiv.org/pdf/1910.06036.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Jingjing Li, Yifan Gao, Lidong Bing, Irwin King, Michael R. Lyu.</em></p>
</li>
<li><p><strong>Weak Supervision Enhanced Generative Network for Question Generation.</strong> IJCAI, 2019. <a href="https://arxiv.org/pdf/1907.00607v1" target="_blank" rel="noopener">paper</a></p>
<p><em>Yutong Wang, Jiyuan Zheng, Qijiong Liu, Zhou Zhao, Jun Xiao, Yueting Zhuang</em></p>
</li>
<li><p><strong>A Multi-Agent Communication Framework for Question-Worthy Phrase Extraction and Question Generation.</strong> AAAI, 2019. <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4700/4578" target="_blank" rel="noopener">paper</a></p>
<p><em>Siyuan Wang, Zhongyu Wei, Zhihao Fan, Yang Liu, Xuanjing Huang</em></p>
</li>
<li><p><strong>Mixture Content Selection for Diverse Sequence Generation.</strong> EMNLP, 2019. <a href="https://arxiv.org/pdf/1909.01953.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/clovaai/FocusSeq2Seq" target="_blank" rel="noopener">code</a></p>
<p><em>Jaemin Cho, Minjoon Seo, Hannaneh Hajishirzi</em></p>
</li>
<li><p><strong>Asking Questions the Human Way: Scalable Question-Answer Generation from Text Corpus.</strong> WWW, 2020. <a href="https://arxiv.org/pdf/2002.00748.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Bang Liu, Haojie Wei, Di Niu, Haolan Chen, Yancheng He</em></p>
</li>
</ol>
<h3 id="Question-Type-Modeling"><a href="#Question-Type-Modeling" class="headerlink" title="Question Type Modeling"></a><a href="#question-type-modeling">Question Type Modeling</a></h3><p>Improve QG by explicitly modeling question types or interrogative words. </p>
<ol>
<li><p><strong>Question Generation for Question Answering.</strong> EMNLP,2017. <a href="https://www.aclweb.org/anthology/D17-1090" target="_blank" rel="noopener">paper</a></p>
<p><em>Nan Duan, Duyu Tang, Peng Chen, Ming Zhou</em></p>
</li>
<li><p><strong>Answer-focused and Position-aware Neural Question Generation.</strong> EMNLP, 2018. <a href="https://www.aclweb.org/anthology/D18-1427" target="_blank" rel="noopener">paper</a></p>
<p><em>Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, Shi Wang</em></p>
</li>
<li><p><strong>Let Me Know What to Ask: Interrogative-Word-Aware Question Generation</strong> EMNLP Workshop, 2019. <a href="https://arxiv.org/pdf/1910.13794.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Junmo Kang, Haritz Puerto San Roman, Sung-Hyon Myaeng</em></p>
</li>
<li><p><strong>Question-type Driven Question Generation</strong> EMNLP, 2019. <a href="https://arxiv.org/pdf/1909.00140.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Wenjie Zhou, Minghua Zhang, Yunfang Wu</em></p>
</li>
</ol>
<h3 id="Encode-Wider-Contexts"><a href="#Encode-Wider-Contexts" class="headerlink" title="Encode Wider Contexts"></a><a href="#encode-wider-contexts">Encode Wider Contexts</a></h3><p>Improve QG by incorporating wider contexts in the input passage. </p>
<ol>
<li><p><strong>Harvesting paragraph-level question-answer pairs from wikipedia.</strong> ACL, 2018. <a href="https://arxiv.org/pdf/1805.05942.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/xinyadu/HarvestingQA" target="_blank" rel="noopener">code&amp;dataset</a></p>
<p> <em>Xinya Du, Claire Cardie</em></p>
</li>
<li><p><strong>Leveraging Context Information for Natural Question Generation</strong> ACL, 2018. <a href="https://www.aclweb.org/anthology/N18-2090" target="_blank" rel="noopener">paper</a> <a href="https://github.com/freesunshine0316/MPQG" target="_blank" rel="noopener">code</a></p>
<p><em>Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, Daniel Gildea</em></p>
</li>
<li><p><strong>Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks.</strong> EMNLP, 2018. <a href="https://www.aclweb.org/anthology/D18-1424.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, Qifa Ke</em></p>
</li>
<li><p><strong>Capturing Greater Context for Question Generation</strong> AAAI, 2020. <a href="https://arxiv.org/pdf/1910.10274.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Luu Anh Tuan, Darsh J Shah, Regina Barzilay</em></p>
</li>
</ol>
<h3 id="Other-Directions"><a href="#Other-Directions" class="headerlink" title="Other Directions"></a><a href="#other-model">Other Directions</a></h3><ol>
<li><p><strong>Generating Question-Answer Hierarchies.</strong> ACL, 2019. <a href="https://arxiv.org/pdf/1906.02622.pdf" target="_blank" rel="noopener">paper</a> <a href="http://squash.cs.umass.edu/" target="_blank" rel="noopener">code</a></p>
<p><em>Kalpesh Krishna and Mohit Iyyer.</em></p>
</li>
<li><p><strong>Unified Language Model Pre-training for Natural Language Understanding and Generation.</strong> NeurIPS, 2019. <a href="https://arxiv.org/pdf/1905.03197.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/microsoft/unilm" target="_blank" rel="noopener">code</a></p>
<p><em>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon</em></p>
</li>
<li><p><strong>Can You Unpack That? Learning to Rewrite Questions-in-Context.</strong> EMNLP, 2019. <a href="https://www.aclweb.org/anthology/D19-1605.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Ahmed Elgohary, Denis Peskov, Jordan L. Boyd-Graber</em></p>
</li>
<li><p><strong>Sequential Copying Networks.</strong> AAAI, 2018. <a href="https://arxiv.org/pdf/1807.02301.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou</em></p>
</li>
<li><p><strong>Let’s Ask Again: Refine Network for Automatic Question Generation.</strong> EMNLP, 2019. <a href="https://www.aclweb.org/anthology/D19-1326.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Preksha Nema, Akash Kumar Mohankumar, Mitesh M. Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran</em></p>
</li>
</ol>
<h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a><a href="#applications">Applications</a></h2><h3 id="Difficulty-Controllable-QG"><a href="#Difficulty-Controllable-QG" class="headerlink" title="Difficulty Controllable QG"></a><a href="#difficulty-controllable-QG">Difficulty Controllable QG</a></h3><p>Endowing the model with the ability to control the difficulty of the generated questions. </p>
<ol>
<li><p><strong>Easy-to-Hard: Leveraging Simple Questions for Complex Question Generation.</strong> arxiv, 2019. <a href="https://arxiv.org/pdf/1912.02367.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Jie Zhao, Xiang Deng, Huan Sun.</em></p>
</li>
<li><p><strong>Difficulty Controllable Generation of Reading Comprehension Questions.</strong> IJCAI, 2019. <a href="https://www.ijcai.org/proceedings/2019/0690.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Yifan Gao, Lidong Bing, Wang Chen, Michael R. Lyu, Irwin King</em> </p>
</li>
<li><p><strong>Difficulty-controllable Multi-hop Question Generation From Knowledge Graphs.</strong> ISWC, 2019. <a href="https://arxiv.org/pdf/1807.03586.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://github.com/liyuanfang/mhqg" target="_blank" rel="noopener">code&amp;dataset</a></p>
<p><em>Vishwajeet Kumar, Yuncheng Hua, Ganesh Ramakrishnan, Guilin Qi, Lianli Gao, Yuan-Fang Li</em></p>
</li>
</ol>
<h3 id="Conversational-QG"><a href="#Conversational-QG" class="headerlink" title="Conversational QG"></a><a href="#conversational-QG">Conversational QG</a></h3><p>Learning to generate a series of coherent questions grounded in a question answering style conversation. </p>
<ol>
<li><p><strong>Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders.</strong> ACL, 2018. <a href="https://arxiv.org/pdf/1805.04843.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/victorywys/Learning2Ask_TypedDecoder" target="_blank" rel="noopener">code</a> <a href="http://coai.cs.tsinghua.edu.cn/hml/dataset/" target="_blank" rel="noopener">dataset</a></p>
<p><em>Yansen Wang, Chenyi Liu, Minlie Huang, Liqiang Nie</em></p>
</li>
<li><p><strong>Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling.</strong> ACL, 2019. <a href="https://arxiv.org/pdf/1906.06893.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/Evan-Gao/conversational-QG" target="_blank" rel="noopener">code</a></p>
<p><em>Yifan Gao, Piji Li, Irwin King, Michael R. Lyu</em></p>
</li>
<li><p><strong>Reinforced Dynamic Reasoning for Conversational Question Generation.</strong> ACL, 2019. <a href="https://www.aclweb.org/anthology/P19-1203" target="_blank" rel="noopener">paper</a> <a href="https://github.com/ZJULearning/ReDR" target="_blank" rel="noopener">code</a> <a href="https://stanfordnlp.github.io/coqa/" target="_blank" rel="noopener">dataset</a></p>
<p><em>Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, Huan Sun</em></p>
</li>
<li><p><strong>Towards Answer-unaware Conversational Question Generation.</strong> ACL Workshop, 2019. <a href="https://www.aclweb.org/anthology/D19-5809.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Mao Nakanishi, Tetsunori Kobayashi, Yoshihiko Hayashi</em></p>
</li>
<li><p><strong>What Should I Ask? Using Conversationally Informative Rewards for Goal-oriented Visual Dialog.</strong> ACL, 2019. <a href="https://www.aclweb.org/anthology/P19-1646.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk, William Yang Wang</em></p>
</li>
<li><p><strong>Visual Dialogue State Tracking for Question Generation.</strong> AAAI, 2020. <a href="https://arxiv.org/pdf/1911.07928.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Wei Pang, Xiaojie Wang</em></p>
</li>
</ol>
<h3 id="Asking-special-questions"><a href="#Asking-special-questions" class="headerlink" title="Asking special questions"></a><a href="#asking-special-questions">Asking special questions</a></h3><p>This direction focuses on exploring how to ask special types of questions, such as mathematical questions, open-ended questions, non-factoid questions, and clarification questions. </p>
<ol>
<li><p><strong>Are You Asking the Right Questions? Teaching Machines to Ask Clarification Questions.</strong> ACL Workshop, 2017. <a href="https://www.aclweb.org/anthology/P17-3006.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Sudha Rao</em></p>
</li>
<li><p><strong>Automatic Opinion Question Generation.</strong> ICNLG, 2018. <a href="https://www.aclweb.org/anthology/W18-6518.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Yllias Chali, Tina Baghaee</em> </p>
</li>
<li><p><strong>A Multi-language Platform for Generating Algebraic Mathematical Word Problems.</strong> arxiv, 2019. <a href="https://arxiv.org/pdf/1912.01110.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Vijini Liyanage, Surangika Ranathunga</em></p>
</li>
<li><p><strong>Interpretation of Natural Language Rules in Conversational Machine Reading.</strong> EMNLP, 2018. <a href="https://arxiv.org/pdf/1809.01494.pdf" target="_blank" rel="noopener">paper</a> <a href="https://sharc-data.github.io/" target="_blank" rel="noopener">dataset</a></p>
<p><em>Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rocktäschel, Mike Sheldon, Guillaume Bouchard, Sebastian Riedel</em></p>
</li>
<li><p><strong>Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums.</strong> ACL, 2019. <a href="https://www.aclweb.org/anthology/P19-1497.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Zi Chai, Xinyu Xing, Xiaojun Wan, Bo Huang</em></p>
</li>
<li><p><strong>Conclusion-Supplement Answer Generation for Non-Factoid Questions.</strong> AAAI, 2020. <a href="https://arxiv.org/pdf/1912.00864.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Makoto Nakatsuji, Sohei Okui</em></p>
</li>
<li><p><strong>Answer-based Adversarial Training for Generating Clarification Questions.</strong> NAACL, 2019. <a href="https://arxiv.org/pdf/1904.02281.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/raosudha89/clarification_question_generation_pytorch" target="_blank" rel="noopener">code</a></p>
<p><em>Rao S, Daumé III H.</em></p>
</li>
<li><p><strong>Distant Supervised Why-Question Generation with Passage Self-Matching Attention.</strong> IJCNN, 2019. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8851781" target="_blank" rel="noopener">paper</a></p>
<p><em>Jiaxin Hu, Zhixu Li, Renshou Wu, Hongling Wang, An Liu, Jiajie Xu, Pengpeng Zhao, Lei Zhao</em></p>
</li>
</ol>
<h3 id="Answer-unaware-QG"><a href="#Answer-unaware-QG" class="headerlink" title="Answer-unaware QG"></a><a href="#answer-unaware-QG">Answer-unaware QG</a></h3><p>In answer-unaware QG, the model does not require the target answer as an input to serve as the focus of asking. Therefore, the model should automatically identify question-worthy parts within the passage to ask. </p>
<ol>
<li><p><strong>Learning to ask: Neural question generation for reading comprehension.</strong> ACL, 2017. <a href="https://www.aclweb.org/anthology/P17-1123.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Xinya Du, Junru Shao, Claire Cardie.</em></p>
</li>
<li><p><strong>Neural Models for Key Phrase Extraction and Question Generation.</strong> ACL Workshop, 2018. <a href="https://www.aclweb.org/anthology/W18-2609.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Sandeep Subramanian, Tong Wang, Xingdi Yuan, Saizheng Zhang, Adam Trischler, Yoshua Bengio</em></p>
</li>
<li><p><strong>Self-Attention Architectures for Answer-Agnostic Neural Question Generation.</strong> ACL, 2019. <a href="https://www.aclweb.org/anthology/P19-1604.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Thomas Scialom, Benjamin Piwowarski, Jacopo Staiano.</em></p>
</li>
</ol>
<h3 id="Unanswerable-QG"><a href="#Unanswerable-QG" class="headerlink" title="Unanswerable QG"></a><a href="#unanswerable-QG">Unanswerable QG</a></h3><p>Learning to generate questions that cannot be answered by the input passage. </p>
<ol>
<li><p><strong>Learning to Ask Unanswerable Questions for Machine Reading Comprehension.</strong> ACL, 2019. <a href="https://www.aclweb.org/anthology/P19-1415.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Haichao Zhu, Li Dong, Furu Wei, Wenhui Wang, Bing Qin, Ting Liu</em></p>
</li>
</ol>
<h3 id="Combining-QA-and-QG"><a href="#Combining-QA-and-QG" class="headerlink" title="Combining QA and QG"></a><a href="#Combining-QA-and-QG">Combining QA and QG</a></h3><p>This direction investigate how to combine the task of QA and QG by multi-task learning or joint training. </p>
<ol>
<li><p><strong>Question Generation for Question Answering.</strong> EMNLP,2017. <a href="https://www.aclweb.org/anthology/D17-1090" target="_blank" rel="noopener">paper</a></p>
<p><em>Nan Duan, Duyu Tang, Peng Chen, Ming Zhou</em></p>
</li>
<li><p><strong>Learning to Collaborate for Question Answering and Asking.</strong> NAACL, 2018. <a href="https://www.aclweb.org/anthology/N18-1141" target="_blank" rel="noopener">paper</a></p>
<p><em>Duyu Tang, Nan Duan, Zhao Yan, Zhirui Zhang, Yibo Sun, Shujie Liu, Yuanhua Lv, Ming Zhou</em></p>
</li>
<li><p><strong>Generating Highly Relevant Questions.</strong> EMNLP, 2019. <a href="https://arxiv.org/abs/1910.03401" target="_blank" rel="noopener">paper</a></p>
<p><em>Jiazuo Qiu, Deyi Xiong</em></p>
</li>
<li><p><strong>Learning to Answer by Learning to Ask: Getting the Best of GPT-2 and BERT Worlds.</strong> arxiv, 2019. <a href="https://arxiv.org/pdf/1911.02365.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Tassilo Klein, Moin Nabi</em></p>
</li>
<li><p><strong>Triple-Joint Modeling for Question Generation Using Cross-Task Autoencoder.</strong> NLPCC, 2019. <a href="https://link.springer.com/chapter/10.1007/978-3-030-32236-6_26" target="_blank" rel="noopener">paper</a></p>
<p><em>Hongling Wang, Renshou Wu, Zhixu Li, Zhongqing Wang, Zhigang Chen, Guodong Zhou</em></p>
</li>
<li><p><strong>Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering</strong> EMNLP, 2019. <a href="https://arxiv.org/pdf/1909.06356.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/ZhangShiyue/QGforQA" target="_blank" rel="noopener">code</a></p>
<p><em>Shiyue Zhang, Mohit Bansal</em></p>
</li>
<li><p><strong>Synthetic QA Corpora Generation with Roundtrip Consistency</strong> ACL, 2019. <a href="https://arxiv.org/pdf/1906.05416.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, Michael Collins</em></p>
</li>
</ol>
<h3 id="QG-from-knowledge-graphs"><a href="#QG-from-knowledge-graphs" class="headerlink" title="QG from knowledge graphs"></a><a href="#QG-from-knowledge-graphs">QG from knowledge graphs</a></h3><p>This direction is about generating questions from a knowledge graph. </p>
<ol>
<li><p><strong>Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus.</strong> ACL, 2016. <a href="https://arxiv.org/pdf/1603.06807.pdf" target="_blank" rel="noopener">paper</a> <a href="https://www.agarciaduran.org" target="_blank" rel="noopener">dataset</a></p>
<p><em>Iulian Vlad Serban, Alberto García-Durán, Çaglar Gülçehre, Sungjin Ahn, Sarath Chandar, Aaron C. Courville, Yoshua Bengio</em></p>
</li>
<li><p><strong>Generating Natural Language Question-Answer Pairs from a Knowledge Graph Using a RNN Based Question Generation Model.</strong> ACL, 2017. <a href="https://www.aclweb.org/anthology/E17-1036/" target="_blank" rel="noopener">paper</a></p>
<p><em>Mitesh M. Khapra, Dinesh Raghu, Sachindra Joshi, Sathish Reddy</em></p>
</li>
<li><p><strong>Knowledge Questions from Knowledge Graphs.</strong> ICTIR, 2017. <a href="https://arxiv.org/pdf/1610.09935.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Dominic Seyler, Mohamed Yahya, Klaus Berberich.</em></p>
</li>
<li><p><strong>Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates and Entity Types.</strong> NAACL, 2018. <a href="https://arxiv.org/pdf/1802.06842.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/NAACL2018Anonymous/submission" target="_blank" rel="noopener">code</a></p>
<p><em>Hady Elsahar, Christophe Gravier, Frederique Laforest.</em></p>
</li>
<li><p><strong>A Neural Question Generation System Based on Knowledge Base</strong> NLPCC, 2018. <a href="https://link.springer.com/chapter/10.1007/978-3-319-99495-6_12" target="_blank" rel="noopener">paper</a></p>
<p><em>Hao Wang, Xiaodong Zhang, Houfeng Wang</em></p>
</li>
<li><p><strong>Formal Query Generation for Question Answering over Knowledge Bases.</strong> ESWC, 2018. <a href="https://link.springer.com/chapter/10.1007/978-3-319-93417-4_46" target="_blank" rel="noopener">paper</a></p>
<p><em>Hamid Zafar, Giulio Napolitano, Jens Lehmann</em></p>
</li>
<li><p><strong>Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss.</strong> EMNLP, 2019. <a href="https://www.aclweb.org/anthology/D19-1247.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Cao Liu, Kang Liu, Shizhu He, Zaiqing Nie, Jun Zhao</em></p>
</li>
<li><p><strong>Difficulty-controllable Multi-hop Question Generation From Knowledge Graphs.</strong> ISWC, 2019. <a href="https://arxiv.org/pdf/1807.03586.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://github.com/liyuanfang/mhqg" target="_blank" rel="noopener">code&amp;dataset</a></p>
<p><em>Vishwajeet Kumar, Yuncheng Hua, Ganesh Ramakrishnan, Guilin Qi, Lianli Gao, Yuan-Fang Li</em></p>
</li>
<li><p><strong>How Question Generation Can Help Question Answering over Knowledge Base.</strong> NLPCC, 2019. <a href="http://tcci.ccf.org.cn/conference/2019/papers/183.pdf" target="_blank" rel="noopener">paper</a></p>
<p> <em>Sen Hu, Lei Zou, Zhanxing Zhu</em></p>
</li>
</ol>
<h3 id="Visual-Question-Generation"><a href="#Visual-Question-Generation" class="headerlink" title="Visual Question Generation"></a><a href="#visual-question-generation">Visual Question Generation</a></h3><p>Asking questions based on visual inputs (usually an image). </p>
<ol>
<li><p><strong>Generating Natural Questions About an Image</strong> ACL, 2016. <a href="https://arxiv.org/pdf/1603.06059.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, Lucy Vanderwende</em></p>
</li>
<li><p><strong>Creativity: Generating Diverse Questions Using Variational Autoencoders</strong> CVPR,2017. <a href="https://arxiv.org/pdf/1704.03493.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Unnat Jain, Ziyu Zhang, Alexander G. Schwing</em></p>
</li>
<li><p><strong>Automatic Generation of Grounded Visual Questions</strong> IJCAI, 2017. <a href="https://www.ijcai.org/proceedings/2017/0592.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, Jiawan Zhang</em></p>
</li>
<li><p><strong>A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators</strong> COLING, 2018. <a href="https://www.aclweb.org/anthology/C18-1150.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, Xuanjing Huang</em></p>
</li>
<li><p><strong>Customized Image Narrative Generation via Interactive Visual Question Generation and Answering</strong> CVPR, 2018. <a href="https://arxiv.org/pdf/1805.00460.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada</em></p>
</li>
<li><p><strong>Multimodal Differential Network for Visual Question Generation</strong> EMNLP, 2018. <a href="https://www.aclweb.org/anthology/D18-1434.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Badri Narayana Patro, Sandeep Kumar, Vinod Kumar Kurmi, Vinay P. Namboodiri</em></p>
</li>
<li><p><strong>A Question Type Driven Framework to Diversify Visual Question Generation</strong> IJCAI, 2018. <a href="http://www.sdspeople.fudan.edu.cn/zywei/paper/fan-ijcai2018.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Zhihao Fan, Zhongyu Wei, Piji Li, Yanyan Lan, Xuanjing Huang</em></p>
</li>
<li><p><strong>Visual Question Generation as Dual Task of Visual Question Answering.</strong> CVPR, 2018. <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Visual_Question_Generation_CVPR_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, Ming Zhou</em> </p>
</li>
<li><p><strong>Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering.</strong> CVPR, 2018. <a href="https://arxiv.org/pdf/1803.11186.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Unnat Jain, Svetlana Lazebnik, Alexander Schwing</em></p>
</li>
<li><p><strong>Information Maximizing Visual Question Generation.</strong> CVPR, 2019. <a href="https://arxiv.org/pdf/1903.11207.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Ranjay Krishna, Michael Bernstein, Li Fei-Fei</em></p>
</li>
<li><p><strong>What Should I Ask? Using Conversationally Informative Rewards for Goal-oriented Visual Dialog.</strong> ACL, 2019. <a href="https://www.aclweb.org/anthology/P19-1646.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk, William Yang Wang</em></p>
</li>
</ol>
<h3 id="Distractor-Generation"><a href="#Distractor-Generation" class="headerlink" title="Distractor Generation"></a><a href="#distractor-generation">Distractor Generation</a></h3><p>Learning to generate distractors for multi-choice questions. </p>
<ol>
<li><p><strong>Generating Questions and Multiple-Choice Answers using Semantic Analysis of Texts.</strong> COLING, 2016. <a href="https://www.aclweb.org/anthology/C16-1107.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Jun Araki, Dheeraj Rajagopal, Sreecharan Sankaranarayanan, Susan Holm, Yukari Yamakawa, Teruko Mitamura</em></p>
</li>
<li><p><strong>Distractor Generation for Multiple Choice Questions Using Learning to Rank.</strong> NAACL Workshop, 2018. <a href="https://www.aclweb.org/anthology/W18-0533.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/harrylclc/LTR-DG" target="_blank" rel="noopener">code</a></p>
<p><em>Chen Liang, Xiao Yang, Neisarg Dave, Drew Wham, Bart Pursel, C. Lee Giles</em></p>
</li>
<li><p><strong>Generating Distractors for Reading Comprehension Questions from Real Examinations.</strong> AAAI, 2019. <a href="https://arxiv.org/pdf/1809.02768.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Yifan Gao, Lidong Bing, Piji Li, Irwin King, Michael R. Lyu</em></p>
</li>
</ol>
<h3 id="Cross-lingual-QG"><a href="#Cross-lingual-QG" class="headerlink" title="Cross-lingual QG"></a><a href="#cross-lingual-QG">Cross-lingual QG</a></h3><p>Building cross-lingual models to generate questions in low-resource languages. </p>
<ol>
<li><p><strong>Cross-Lingual Training for Automatic Question Generation.</strong> ACL, 2019. <a href="https://arxiv.org/pdf/1906.02525.pdf" target="_blank" rel="noopener">paper</a> <a href="https://www.cse.iitb.ac.in/~ganesh/HiQuAD/clqg/" target="_blank" rel="noopener">dataset</a></p>
<p><em>Vishwajeet Kumar, Nitish Joshi, Arijit Mukherjee, Ganesh Ramakrishnan, Preethi Jyothi</em></p>
</li>
<li><p><strong>Cross-Lingual Natural Language Generation via Pre-Training.</strong> AAAI, 2020. <a href="https://arxiv.org/pdf/1909.10481.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, Heyan Huang</em></p>
</li>
</ol>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a><a href="#evaluation">Evaluation</a></h2><p>This direction investigates the mechanism behind question asking, and how to evaluate the quality of generated questions. </p>
<ol>
<li><p><strong>Question Asking as Program Generation.</strong> NeurIPS, 2017. <a href="https://arxiv.org/pdf/1711.06351.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Anselm Rothe, Brenden M. Lake, Todd M. Gureckis.</em></p>
</li>
<li><p><strong>Towards a Better Metric for Evaluating Question Generation Systems.</strong> EMNLP, 2018. <a href="https://www.aclweb.org/anthology/D18-1429/" target="_blank" rel="noopener">paper</a></p>
<p><em>Preksha Nema, Mitesh M. Khapra.</em></p>
</li>
<li><p><strong>Evaluating Rewards for Question Generation Models.</strong> NAACL, 2019. <a href="https://arxiv.org/pdf/1902.11049.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Tom Hosking and Sebastian Riedel.</em></p>
</li>
</ol>
<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a><a href="#resources">Resources</a></h2><p>QG-specific datasets and toolkits. </p>
<ol>
<li><p><strong>LearningQ: A Large-Scale Dataset for Educational Question Generation.</strong> ICWSM, 2018. <a href="https://yangjiera.github.io/works/icwsm2018.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Guanliang Chen, Jie Yang, Claudia Hauff, Geert-Jan Houben.</em></p>
</li>
<li><p><strong>ParaQG: A System for Generating Questions and Answers from Paragraphs.</strong> EMNLP Demo, 2019. <a href="https://arxiv.org/pdf/1909.01642.pdf" target="_blank" rel="noopener">paper</a></p>
<p><em>Vishwajeet Kumar, Sivaanandh Muneeswaran, Ganesh Ramakrishnan, Yuan-Fang Li.</em></p>
</li>
<li><p><strong>How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions.</strong> AAAI, 2020. <a href="https://arxiv.org/pdf/1911.09247.pdf" target="_blank" rel="noopener">paper</a> <a href="https://github.com/ZeweiChu/MQR" target="_blank" rel="noopener">code</a></p>
<p><em>Zewei Chu, Mingda Chen, Jing Chen, Miaosen Wang, Kevin Gimpel, Manaal Faruqui, Xiance Si.</em></p>
</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-04-08-Study-Data-Science-QA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-04-08-Study-Data-Science-QA/" itemprop="url">Data Science Question and Answer</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-09T08:00:00+08:00">
                2020-04-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>The purpose of this repo is two fold:</p>
<ul>
<li>To help you (data science practitioners) prepare for data science related interviews</li>
<li>To introduce to people who don’t know but want to learn some basic data science concepts</li>
</ul>
<p>The focus is on the knowledge breadth so this is more of a quick reference rather than an in-depth study material. If you want to learn a specific topic in detail please refer to other content or reach out and I’d love to point you to materials I found useful.</p>
<p>I might add some topics from time to time but hey, this should also be a community effort, right? Any pull request is welcome!</p>
<p>Here are the categorizes:</p>
<ul>
<li><a href="#resume">Resume</a></li>
<li><a href="#sql">SQL</a></li>
<li><a href="#tools-and-framework">Tools and Framework</a></li>
<li><a href="#statistics-and-ml-in-general">Statistics and ML In General</a></li>
<li><a href="#supervised-learning">Supervised Learning</a></li>
<li><a href="#unsupervised-learning">Unsupervised Learning</a></li>
<li><a href="#reinforcement-learning">Reinforcement Learning</a></li>
<li><a href="#natural-language-processing">Natural Language Processing</a></li>
<li><a href="#system">System</a></li>
</ul>
<h2 id="Resume"><a href="#Resume" class="headerlink" title="Resume"></a>Resume</h2><p>The only advice I can give about resume is to indicate your past data science / machine learning projects in a specific, <strong>quantifiable</strong> way. Consider the following two statements:</p>
<blockquote>
<p>Trained a machine learning system</p>
</blockquote>
<p>and</p>
<blockquote>
<p>Designed and deployed a deep learning model to recognize objects using Keras, Tensorflow, and Node.js. The model has 1/30 model size, 1/3 training time, 1/5 inference time, and 2x faster convergence compared with traditional neural networks (e.g, ResNet)</p>
</blockquote>
<p>The second is much better because it quantifies your contribution and also highlights specific technologies you used (and therefore have expertise in). This would require you to log what you’ve done during experiments. But don’t exaggerate.</p>
<p>Spend some time going over your resume / past projects to make sure you explain them well.</p>
<h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><ul>
<li><a href="#difference-between-joins">Difference between joins</a></li>
</ul>
<h3 id="Difference-between-joins"><a href="#Difference-between-joins" class="headerlink" title="Difference between joins"></a>Difference between joins</h3><ul>
<li><strong>(INNER) JOIN</strong>: Returns records that have matching values in both tables</li>
<li><strong>LEFT (OUTER) JOIN</strong>: Return all records from the left table, and the matched records from the right table</li>
<li><strong>RIGHT (OUTER) JOIN</strong>: Return all records from the right table, and the matched records from the left table</li>
<li><strong>FULL (OUTER) JOIN</strong>: Return all records when there is a match in either left or right table</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/sql-join.PNG" alt="link"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h2 id="Tools-and-Framework"><a href="#Tools-and-Framework" class="headerlink" title="Tools and Framework"></a>Tools and Framework</h2><p>The resources here are only meant to help you brush up on the topis rather than making you an expert.</p>
<ul>
<li><a href="#spark">Spark</a></li>
</ul>
<h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Using PySpark API.</p>
<ul>
<li>The best resource is of course <a href="https://spark.apache.org/docs/latest/" target="_blank" rel="noopener">Spark’s documentation</a>. Take a thorough review of the topics</li>
<li>If you are really time constrained, scan the Spark’s documentation and check <a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf" target="_blank" rel="noopener">PySpark cheat sheet</a> for the basics</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h2 id="Statistics-and-ML-In-General"><a href="#Statistics-and-ML-In-General" class="headerlink" title="Statistics and ML In General"></a>Statistics and ML In General</h2><ul>
<li><a href="#resume">Resume</a></li>
<li><a href="#sql">SQL</a><ul>
<li><a href="#difference-between-joins">Difference between joins</a></li>
</ul>
</li>
<li><a href="#tools-and-framework">Tools and Framework</a><ul>
<li><a href="#spark">Spark</a></li>
</ul>
</li>
<li><a href="#statistics-and-ml-in-general">Statistics and ML In General</a><ul>
<li><a href="#project-workflow">Project Workflow</a></li>
<li><a href="#cross-validation">Cross Validation</a></li>
<li><a href="#feature-importance">Feature Importance</a></li>
<li><a href="#mean-squared-error-vs-mean-absolute-error">Mean Squared Error vs. Mean Absolute Error</a></li>
<li><a href="#l1-vs-l2-regularization">L1 vs L2 regularization</a></li>
<li><a href="#correlation-vs-covariance">Correlation vs Covariance</a></li>
<li><a href="#would-adding-more-data-address-underfitting">Would adding more data address underfitting</a></li>
<li><a href="#activation-function">Activation Function</a></li>
<li><a href="#bagging">Bagging</a></li>
<li><a href="#stacking">Stacking</a></li>
<li><a href="#generative-vs-discriminative">Generative vs discriminative</a></li>
<li><a href="#parametric-vs-nonparametric">Parametric vs Nonparametric</a></li>
<li><a href="#recommender-system">Recommender System</a></li>
</ul>
</li>
<li><a href="#supervised-learning">Supervised Learning</a><ul>
<li><a href="#linear-regression">Linear regression</a></li>
<li><a href="#logistic-regression">Logistic regression</a></li>
<li><a href="#naive-bayes">Naive Bayes</a></li>
<li><a href="#knn">KNN</a></li>
<li><a href="#svm">SVM</a></li>
<li><a href="#decision-tree">Decision tree</a></li>
<li><a href="#random-forest">Random forest</a></li>
<li><a href="#boosting-tree">Boosting Tree</a></li>
<li><a href="#mlp">MLP</a></li>
<li><a href="#cnn">CNN</a></li>
<li><a href="#rnn-and-lstm">RNN and LSTM</a></li>
</ul>
</li>
<li><a href="#unsupervised-learning">Unsupervised Learning</a><ul>
<li><a href="#clustering">Clustering</a></li>
<li><a href="#principal-component-analysis">Principal Component Analysis</a></li>
<li><a href="#autoencoder">Autoencoder</a></li>
<li><a href="#generative-adversarial-network">Generative Adversarial Network</a></li>
</ul>
</li>
<li><a href="#reinforcement-learning">Reinforcement Learning</a></li>
<li><a href="#natural-language-processing">Natural Language Processing</a><ul>
<li><a href="#tokenization">Tokenization</a></li>
<li><a href="#stemming-and-lemmatization">Stemming and lemmatization</a></li>
<li><a href="#n-gram">N gram</a></li>
<li><a href="#bag-of-words">Bag of Words</a></li>
<li><a href="#word2vec">word2vec</a></li>
</ul>
</li>
<li><a href="#system">System</a><ul>
<li><a href="#cron-job">Cron job</a></li>
<li><a href="#linux">Linux</a></li>
</ul>
</li>
</ul>
<h3 id="Project-Workflow"><a href="#Project-Workflow" class="headerlink" title="Project Workflow"></a>Project Workflow</h3><p>Given a data science / machine learning project, what steps should we follow? Here’s<br>how I would tackle it:</p>
<ul>
<li><strong>Specify business objective.</strong> Are we trying to win more customers, achieve higher satisfaction, or gain more revenues?</li>
<li><strong>Define problem.</strong> What is the specific gap in your ideal world and the real one that requires machine learning to fill? Ask questions that can be addressed using your data and predictive modeling (ML algorithms).</li>
<li><strong>Create a common sense baseline.</strong> But before you resort to ML, set up a baseline to solve the problem as if you know zero data science. You may be amazed at how effective this baseline is. It can be as simple as recommending the top N popular items or other rule-based logic. This baseline can also server as a good benchmark for ML algorithms.</li>
<li><strong>Review ML literatures.</strong> To avoid reinventing the wheel and get inspired on what techniques / algorithms are good at addressing the questions using our data.</li>
<li><strong>Set up a single-number metric.</strong> What it means to be successful - high accuracy, lower error, or bigger AUC - and how do you measure it? The metric has to align with high-level goals, most often the success of your business. Set up a single-number against which all models are measured.</li>
<li><strong>Do exploratory data analysis (EDA).</strong> Play with the data to get a general idea of data type, distribution, variable correlation, facets etc. This step would involve a lot of plotting.</li>
<li><strong>Partition data.</strong> Validation set should be large enough to detect differences between the models you are training; test set should be large enough to indicate the overall performance of the final model; training set, needless to say, the larger the merrier.</li>
<li><strong>Preprocess.</strong> This would include data integration, cleaning, transformation, reduction, discretization and more.</li>
<li><strong>Engineer features.</strong> Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering. This step usually involves feature selection and creation, using domain knowledge. Can be minimal for deep learning projects.</li>
<li><strong>Develop models.</strong> Choose which algorithm to use, what hyperparameters to tune, which architecture to use etc.</li>
<li><strong>Ensemble.</strong> Ensemble can usually boost performance, depending on the correlations of the models/features. So it’s always a good idea to try out. But be open-minded about making tradeoff - some ensemble are too complex/slow to put into production.</li>
<li><strong>Deploy model.</strong> Deploy models into production for inference.</li>
<li><strong>Monitor model.</strong> Monitor model performance, and collect feedbacks.</li>
<li><strong>Iterate.</strong> Iterate the previous steps. Data science tends to be an iterative process, with new and improved models being developed over time.</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/workflow.png" alt="link"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h3><p>Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a validation set to evaluate it. For example, a k-fold cross validation divides the data into k folds (or partitions), trains on each k-1 fold, and evaluate on the remaining 1 fold. This results to k models/evaluations, which can be averaged to get a overall model performance.</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/cv.png" alt="link"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Feature-Importance"><a href="#Feature-Importance" class="headerlink" title="Feature Importance"></a>Feature Importance</h3><ul>
<li>In linear models, feature importance can be calculated by the scale of the coefficients</li>
<li>In tree-based methods (such as random forest), important features are likely to appear closer to the root of the tree.  We can get a feature’s importance for random forest by computing the averaging depth at which it appears across all trees in the forest.</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Mean-Squared-Error-vs-Mean-Absolute-Error"><a href="#Mean-Squared-Error-vs-Mean-Absolute-Error" class="headerlink" title="Mean Squared Error vs. Mean Absolute Error"></a>Mean Squared Error vs. Mean Absolute Error</h3><ul>
<li><strong>Similarity</strong>: both measure the average model prediction error; range from 0 to infinity; the lower the better</li>
<li>Mean Squared Error (MSE) gives higher weights to large error (e.g., being off by 10 just MORE THAN TWICE as bad as being off by 5), whereas Mean Absolute Error (MAE) assign equal weights (being off by 10 is just twice as bad as being off by 5)</li>
<li>MSE is continuously differentiable, MAE is not (where y_pred == y_true)</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="L1-vs-L2-regularization"><a href="#L1-vs-L2-regularization" class="headerlink" title="L1 vs L2 regularization"></a>L1 vs L2 regularization</h3><ul>
<li><strong>Similarity</strong>: both L1 and L2 regularization <strong>prevent overfitting</strong> by shrinking (imposing a penalty) on the coefficients</li>
<li><strong>Difference</strong>: L2 (Ridge) shrinks all the coefficient by the same proportions but eliminates none, while L1 (Lasso) can shrink some coefficients to zero, performing variable selection.</li>
<li><strong>Which to choose</strong>: If all the features are correlated with the label, ridge outperforms lasso, as the coefficients are never zero in ridge. If only a subset of features are correlated with the label, lasso outperforms ridge as in lasso model some coefficient can be shrunken to zero.</li>
<li>In Graph (a), the black square represents the feasible region of the L1 regularization while graph (b) represents the feasible region for L2 regularization. The contours in the plots represent different loss values (for the unconstrained regression model ). The feasible point that minimizes the loss is more likely to happen on the coordinates on graph (a) than on graph (b) since graph (a) is more <strong>angular</strong>.  This effect amplifies when your number of coefficients increases, i.e. from 2 to 200. The implication of this is that the L1 regularization gives you sparse estimates. Namely, in a high dimensional space, you got mostly zeros and a small number of non-zero coefficients.</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/l1l2.png" alt="link"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Correlation-vs-Covariance"><a href="#Correlation-vs-Covariance" class="headerlink" title="Correlation vs Covariance"></a>Correlation vs Covariance</h3><ul>
<li>Both determine the relationship and measure the dependency between two random variables</li>
<li>Correlation is when the change in one item may result in the change in the another item, while covariance is when two items vary together (joint variability)</li>
<li>Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance</li>
<li>Range: correlation is between -1 and +1, while covariance lies between negative infinity and infinity.</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Would-adding-more-data-address-underfitting"><a href="#Would-adding-more-data-address-underfitting" class="headerlink" title="Would adding more data address underfitting"></a>Would adding more data address underfitting</h3><p>Underfitting happens when a model is not complex enough to learn well from the data. It is the problem of model rather than data size. So a potential way to address underfitting is to increase the model complexity (e.g., to add higher order coefficients for linear model, increase depth for tree-based methods, add more layers / number of neurons for neural networks etc.)</p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h3><p>For neural networks</p>
<ul>
<li>Non-linearity: ReLU is often used. Use Leaky ReLU (a small positive gradient for negative input, say, <code>y = 0.01x</code> when x &lt; 0) to address dead ReLU issue</li>
<li>Multi-class: softmax</li>
<li>Binary: sigmoid</li>
<li>Regression: linear</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>To address overfitting, we can use an ensemble method called bagging (bootstrap aggregating),<br>which reduces the variance of the meta learning algorithm. Bagging can be applied<br>to decision tree or other algorithms.</p>
<p>Here is a <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py" target="_blank" rel="noopener">great illustration</a> of a single estimator vs. bagging.</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/bagging.png" alt="link"></p>
<ul>
<li>Bagging is when samlping is performed <em>with</em> replacement. When sampling is performed <em>without</em> replacement, it’s called pasting.</li>
<li>Bagging is popular due to its boost for performance, but also due to that individual learners can be trained in parallel and scale well</li>
<li>Ensemble methods work best when the learners are as independent from one another as possible</li>
<li>Voting: soft voting (predict probability and average over all individual learners) often works better than hard voting</li>
<li>out-of-bag instances can act validation set for bagging</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><ul>
<li>Instead of using trivial functions (such as hard voting) to aggregate the predictions from individual learners, train a model to perform this aggregation</li>
<li>First split the training set into two subsets: the first subset is used to train the learners in the first layer</li>
<li>Next the first layer learners are used to make predictions (meta features) on the second subset, and those predictions are used to train another models (to obtain the weigts of different learners) in the second layer</li>
<li>We can train multiple models in the second layer, but this entails subsetting the original dataset into 3 parts</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/stacking.jpg" alt="stacking"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Generative-vs-discriminative"><a href="#Generative-vs-discriminative" class="headerlink" title="Generative vs discriminative"></a>Generative vs discriminative</h3><ul>
<li>Discriminative algorithms model <em>p(y|x; w)</em>, that is, given the dataset and learned<br>parameter, what is the probability of y belonging to a specific class. A discriminative algorithm<br>doesn’t care about how the data was generated, it simply categorizes a given example</li>
<li>Generative algorithms try to model <em>p(x|y)</em>, that is, the distribution of features given<br>that it belongs to a certain class. A generative algorithm models how the data was<br>generated.</li>
</ul>
<blockquote>
<p>Given a training set, an algorithm like logistic regression or<br>the perceptron algorithm (basically) tries to find a straight line—that is, a<br>decision boundary—that separates the elephants and dogs. Then, to classify<br>a new animal as either an elephant or a dog, it checks on which side of the<br>decision boundary it falls, and makes its prediction accordingly.</p>
<p>Here’s a different approach. First, looking at elephants, we can build a<br>model of what elephants look like. Then, looking at dogs, we can build a<br>separate model of what dogs look like. Finally, to classify a new animal, we<br>can match the new animal against the elephant model, and match it against<br>the dog model, to see whether the new animal looks more like the elephants<br>or more like the dogs we had seen in the training set.</p>
</blockquote>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Parametric-vs-Nonparametric"><a href="#Parametric-vs-Nonparametric" class="headerlink" title="Parametric vs Nonparametric"></a>Parametric vs Nonparametric</h3><ul>
<li>A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model.</li>
<li>A model where the number of parameters is not determined prior to training. Nonparametric does not mean that they have no parameters. On the contrary, nonparametric models (can) become more and more complex with an increasing amount of data.</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Recommender-System"><a href="#Recommender-System" class="headerlink" title="Recommender System"></a>Recommender System</h3><ul>
<li>I put recommend system here since technically it falls neither under supervised nor unsupervised learning</li>
<li>A recommender system seeks to predict the ‘rating’ or ‘preference’ a user would give to items and then recommend items accordingly</li>
<li>Content based recommender systems recommends items similar to those a given user has liked in the past, based on either explicit (ratings, like/dislike button) or implicit (viewed/finished an article) feedbacks. Content based recommenders work solely with the past interactions of a given user and do not take other users into consideration.</li>
<li>Collaborative filtering is based on past interactions of the whole user base. There are two Collaborative filtering approaches: <strong>item-based</strong> or <strong>user-based</strong><ul>
<li>item-based: for user u, a score for an unrated item is produced by combining the ratings of users similar to u.</li>
<li>user-based:  a rating (u, i) is produced by looking at the set of items similar to i (interaction similarity), then the ratings by u of similar items are combined into a predicted rating</li>
</ul>
</li>
<li>In recommender systems traditionally matrix factorization methods are used, although we recently there are also deep learning based methods</li>
<li>Cold start and sparse matrix can be issues for recommender systems</li>
<li>Widely used in movies, news, research articles, products, social tags, music, etc.</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/collaborative_filtering.gif" alt="cf"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><ul>
<li><a href="#linear-regression">Linear regression</a></li>
<li><a href="#logistic-regression">Logistic regression</a></li>
<li><a href="#naive-bayes">Naive Bayes</a></li>
<li><a href="#knn">KNN</a></li>
<li><a href="#svm">SVM</a></li>
<li><a href="#decision-tree">Decision tree</a></li>
<li><a href="#random-forest">Random forest</a></li>
<li><a href="#boosting-tree">Boosting Tree</a></li>
<li><a href="#mlp">MLP</a></li>
<li><a href="#cnn">CNN</a></li>
<li><a href="#rnn-and-lstm">RNN and LSTM</a></li>
</ul>
<h3 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear regression"></a>Linear regression</h3><ul>
<li>How to learn the parameter: minimize the cost function</li>
<li>How to minimize cost function: gradient descent</li>
<li>Regularization:<ul>
<li>L1 (Lasso): can shrink certain coef to zero, thus performing feature selection</li>
<li>L2 (Ridge): shrink all coef with the same proportion; almost always outperforms L1</li>
<li>Elastic Net: combined L1 and L2 priors as regularizer</li>
</ul>
</li>
<li>Assumes linear relationship between features and the label</li>
<li>Can add polynomial and interaction features to add non-linearity</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/lr.png" alt="lr"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h3><ul>
<li>Generalized linear model (GLM) for binary classification problems</li>
<li>Apply the sigmoid function to the output of linear models, squeezing the target<br>to range [0, 1]</li>
<li>Threshold to make prediction: usually if the output &gt; .5, prediction 1; otherwise prediction 0</li>
<li>A special case of softmax function, which deals with multi-class problems</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><ul>
<li>Naive Bayes (NB) is a supervised learning algorithm based on applying <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener">Bayes’ theorem</a></li>
<li>It is called naive because it builds the naive assumption that each feature<br>are independent of each other</li>
<li>NB can make different assumptions (i.e., data distributions, such as Gaussian,<br>Multinomial, Bernoulli)</li>
<li>Despite the over-simplified assumptions, NB classifier works quite well in real-world<br>applications, especially for text classification (e.g., spam filtering)</li>
<li>NB can be extremely fast compared to more sophisticated methods</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><ul>
<li>Given a data point, we compute the K nearest data points (neighbors) using certain<br>distance metric (e.g., Euclidean metric). For classification, we take the majority label<br>of neighbors; for regression, we take the mean of the label values.</li>
<li>Note for KNN we don’t train a model; we simply compute during<br>inference time. This can be computationally expensive since each of the test example<br>need to be compared with every training example to see how close they are.</li>
<li>There are approximation methods can have faster inference time by<br>partitioning the training data into regions (e.g., <a href="https://github.com/spotify/annoy" target="_blank" rel="noopener">annoy</a>)</li>
<li>When K equals 1 or other small number the model is prone to overfitting (high variance), while<br>when K equals number of data points or other large number the model is prone to underfitting (high bias)</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/knn.png" alt="KNN"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><ul>
<li>Can perform linear, nonlinear, or outlier detection (unsupervised)</li>
<li>Large margin classifier: using SVM we not only have a decision boundary, but want the boundary<br>to be as far from the closest training point as possible</li>
<li>The closest training examples are called support vectors, since they are the points<br>based on which the decision boundary is drawn</li>
<li>SVMs are sensitive to feature scaling</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/svm.png" alt="svm"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Decision-tree"><a href="#Decision-tree" class="headerlink" title="Decision tree"></a>Decision tree</h3><ul>
<li>Non-parametric, supervised learning algorithms</li>
<li>Given the training data, a decision tree algorithm divides the feature space into<br>regions. For inference, we first see which<br>region does the test data point fall in, and take the mean label values (regression)<br>or the majority label value (classification).</li>
<li><strong>Construction</strong>: top-down, chooses a variable to split the data such that the<br>target variables within each region are as homogeneous as possible. Two common<br>metrics: gini impurity or information gain, won’t matter much in practice.</li>
<li>Advantage: simply to understand &amp; interpret, mirrors human decision making</li>
<li>Disadvantage:<ul>
<li>can overfit easily (and generalize poorly) if we don’t limit the depth of the tree</li>
<li>can be non-robust: A small change in the training data can lead to a totally different tree</li>
<li>instability: sensitive to training set rotation due to its orthogonal decision boundaries</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/tree.gif" alt="decision tree"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Random-forest"><a href="#Random-forest" class="headerlink" title="Random forest"></a>Random forest</h3><p>Random forest improves bagging further by adding some randomness. In random forest,<br>only a subset of features are selected at random to construct a tree (while often not subsample instances).<br>The benefit is that random forest <strong>decorrelates</strong> the trees.</p>
<p>For example, suppose we have a dataset. There is one very predicative feature, and a couple<br>of moderately predicative features. In bagging trees, most of the trees<br>will use this very predicative feature in the top split, and therefore making most of the trees<br>look similar, <strong>and highly correlated</strong>. Averaging many highly correlated results won’t lead<br>to a large reduction in variance compared with uncorrelated results.<br>In random forest for each split we only consider a subset of the features and therefore<br>reduce the variance even further by introducing more uncorrelated trees.</p>
<p>I wrote a <a href="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/bag-rf-var.ipynb" target="_blank" rel="noopener">notebook</a> to illustrate this point.</p>
<p>In practice, tuning random forest entails having a large number of trees (the more the better, but<br>always consider computation constraint). Also, <code>min_samples_leaf</code> (The minimum number of<br>samples at the leaf node)to control the tree size and overfitting. Always cross validate the parameters.</p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Boosting-Tree"><a href="#Boosting-Tree" class="headerlink" title="Boosting Tree"></a>Boosting Tree</h3><p><strong>How it works</strong></p>
<p>Boosting builds on weak learners, and in an iterative fashion. In each iteration,<br>a new learner is added, while all existing learners are kept unchanged. All learners<br>are weighted based on their performance (e.g., accuracy), and after a weak learner<br>is added, the data are re-weighted: examples that are misclassified gain more weights,<br>while examples that are correctly classified lose weights. Thus, future weak learners<br>focus more on examples that previous weak learners misclassified.</p>
<p><strong>Difference from random forest (RF)</strong></p>
<ul>
<li>RF grows trees <strong>in parallel</strong>, while Boosting is sequential</li>
<li>RF reduces variance, while Boosting reduces errors by reducing bias</li>
</ul>
<p><strong>XGBoost (Extreme Gradient Boosting)</strong></p>
<blockquote>
<p>XGBoost uses a more regularized model formalization to control overfitting, which gives it better performance</p>
</blockquote>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p>A feedforward neural network of multiple layers. In each layer we<br>can have multiple neurons, and each of the neuron in the next layer is a linear/nonlinear<br>combination of the all the neurons in the previous layer. In order to train the network<br>we back propagate the errors layer by layer. In theory MLP can approximate any functions.</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/mlp.jpg" alt="mlp"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>The Conv layer is the building block of a Convolutional Network. The Conv layer consists<br>of a set of learnable filters (such as 5 <em> 5 </em> 3, width <em> height </em> depth). During the forward<br>pass, we slide (or more precisely, convolve) the filter across the input and compute the dot<br>product. Learning again happens when the network back propagate the error layer by layer.</p>
<p>Initial layers capture low-level features such as angle and edges, while later<br>layers learn a combination of the low-level features and in the previous layers<br>and can therefore represent higher level feature, such as shape and object parts.</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/cnn.jpg" alt="CNN"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="RNN-and-LSTM"><a href="#RNN-and-LSTM" class="headerlink" title="RNN and LSTM"></a>RNN and LSTM</h3><p>RNN is another paradigm of neural network where we have difference layers of cells,<br>and each cell not only takes as input the cell from the previous layer, but also the previous<br>cell within the same layer. This gives RNN the power to model sequence.</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/rnn.jpeg" alt="RNN"></p>
<p>This seems great, but in practice RNN barely works due to exploding/vanishing gradient, which<br>is cause by a series of multiplication of the same matrix. To solve this, we can use<br>a variation of RNN, called long short-term memory (LSTM), which is capable of learning<br>long-term dependencies.</p>
<p>The math behind LSTM can be pretty complicated, but intuitively LSTM introduce</p>
<ul>
<li>input gate</li>
<li>output gate</li>
<li>forget gate</li>
<li>memory cell (internal state)</li>
</ul>
<p>LSTM resembles human memory: it forgets old stuff (old internal state <em> forget gate)<br>and learns from new input (input node </em> input gate)</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/lstm.png" alt="lstm"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><ul>
<li><a href="#clustering">Clustering</a></li>
<li><a href="#principal-component-analysis">Principal Component Analysis</a></li>
<li><a href="#autoencoder">Autoencoder</a></li>
<li><a href="#generative-adversarial-network">Generative Adversarial Network</a></li>
</ul>
<h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><ul>
<li>Clustering is a unsupervised learning algorithm that groups data in such<br>a way that data points in the same group are more similar to each other than to<br>those from other groups</li>
<li>Similarity is usually defined using a distance measure (e.g, Euclidean, Cosine, Jaccard, etc.)</li>
<li>The goal is usually to discover the underlying structure within the data (usually high dimensional)</li>
<li>The most common clustering algorithm is K-means, where we define K (the number of clusters)<br>and the algorithm iteratively finds the cluster each data point belongs to</li>
</ul>
<p><a href="http://scikit-learn.org/stable/modules/clustering.html" target="_blank" rel="noopener">scikit-learn</a> implements many clustering algorithms. Below is a comparison adopted from its page.</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/clustering.png" alt="clustering"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Principal-Component-Analysis"><a href="#Principal-Component-Analysis" class="headerlink" title="Principal Component Analysis"></a>Principal Component Analysis</h3><ul>
<li>Principal Component Analysis (PCA) is a dimension reduction technique that projects<br>the data into a lower dimensional space</li>
<li>PCA uses Singular Value Decomposition (SVD), which is a matrix factorization method<br>that decomposes a matrix into three smaller matrices (more details of SVD <a href="https://en.wikipedia.org/wiki/Singular-value_decomposition" target="_blank" rel="noopener">here</a>)</li>
<li>PCA finds top N principal components, which are dimensions along which the data vary<br>(spread out) the most. Intuitively, the more spread out the data along a specific dimension,<br>the more information is contained, thus the more important this dimension is for the<br>pattern recognition of the dataset</li>
<li>PCA can be used as pre-step for data visualization: reducing high dimensional data<br>into 2D or 3D. An alternative dimensionality reduction technique is <a href="https://lvdmaaten.github.io/tsne/" target="_blank" rel="noopener">t-SNE</a></li>
</ul>
<p>Here is a visual explanation of PCA</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/pca.gif" alt="pca"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><ul>
<li>The aim of an autoencoder is to learn a representation (encoding) for a set of data</li>
<li>An autoencoder always consists of two parts, the encoder and the decoder. The encoder would find a lower dimension representation (latent variable) of the original input, while the decoder is used to reconstruct from the lower-dimension vector such that the distance between the original and reconstruction is minimized</li>
<li>Can be used for data denoising and dimensionality reduction</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/autoencoder.png" alt="link"></p>
<h3 id="Generative-Adversarial-Network"><a href="#Generative-Adversarial-Network" class="headerlink" title="Generative Adversarial Network"></a>Generative Adversarial Network</h3><ul>
<li>Generative Adversarial Network (GAN) is an unsupervised learning algorithm that also has supervised flavor: using supervised loss as part of training</li>
<li>GAN typically has two major components: the <strong>generator</strong> and the <strong>discriminator</strong>. The generator tries to generate “fake” data (e.g, images or sentences) that fool the discriminator into thinking that they’re real, while the discriminator tries to distinguish between real and generated data. It’s a fight between the two players thus the name adversarial, and this fight drives both sides to improve until “fake” data are indistinguishable from the real data</li>
<li>How does it work, intuitively<ul>
<li>The generator takes a <strong>random</strong> input and generates a sample of data</li>
<li>The discriminator then either takes the generated sample or a real data sample, and tries to predict whether the input is real or generated (i.e., solving a binary classification problem)</li>
<li>Given a truth score range of [0, 1], ideally the we’d love to see discriminator give low score to generated data but high score to real data. On the other hand, we also wanna see the generated data fool the discriminator. And this paradox drives both sides become stronger</li>
</ul>
</li>
<li>How does it work, from a training perspective<ul>
<li>Without training, the generator creates ‘garbage’ data only while the discriminator is too ‘innocent’ to tell the difference between fake and real data</li>
<li>Usually we would first train the discriminator with both real (label 1) and generated data (label 0) for N epochs so it would have a good judgement of what is real vs. fake</li>
<li>Then we <strong>set the discriminator non-trainable</strong>, and train the generator. Even though the discriminator is non-trainable at this stage, we still use it as a classifier so that <strong>error signals can be back propagated and therefore enable the generator to learn</strong></li>
<li>The above two steps would continue in turn until both sides cannot be improved further</li>
</ul>
</li>
<li>Here are some <a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">tips and tricks to make GANs work</a></li>
<li>One Caveat is that the <strong>adversarial part is only auxiliary: The end goal of using GAN is to generate data that even experts cannot tell if it’s real or fake</strong></li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/gan.jpg" alt="gan"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p>[TODO]</p>
<h2 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h2><ul>
<li><a href="#tokenization">Tokenization</a></li>
<li><a href="#stemming-and-lemmatization">Stemming and lemmatization</a></li>
<li><a href="#ngram">N-gram</a></li>
<li><a href="#bag-of-words">Bag of Words</a></li>
<li><a href="#word2vec">word2vec</a></li>
</ul>
<h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><ul>
<li>Tokenization is the process of converting a sequence of characters into a sequence of tokens</li>
<li>Consider this example: <code>The quick brown fox jumped over the lazy dog</code>. In this case each word (separated by space) would be a token</li>
<li>Sometimes tokenization doesn’t have a definitive answer. For instance, <code>O&#39;Neill</code> can be tokenized to <code>o</code> and <code>neill</code>, <code>oneill</code>, or <code>o&#39;neill</code>.</li>
<li>In some cases tokenization requires language-specific knowledge. For example, it doesn’t make sense to tokenize <code>aren&#39;t</code> into <code>aren</code> and <code>t</code></li>
<li>For a more detailed treatment of tokenization please check <a href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" target="_blank" rel="noopener">here</a></li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Stemming-and-lemmatization"><a href="#Stemming-and-lemmatization" class="headerlink" title="Stemming and lemmatization"></a>Stemming and lemmatization</h3><ul>
<li>The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form</li>
<li>Stemming usually refers to a crude heuristic process that chops off the ends of words</li>
<li>Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words</li>
<li>If confronted with the token <code>saw</code>, stemming might return just <code>s</code>, whereas lemmatization would attempt to return either <code>see</code> or <code>saw</code> depending on whether the use of the token was as a verb or a noun</li>
<li>For a more detailed treatment please check <a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html" target="_blank" rel="noopener">here</a></li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="N-gram"><a href="#N-gram" class="headerlink" title="N gram"></a>N gram</h3><ul>
<li>n-gram is a contiguous sequence of n items from a given sample of text or speech</li>
<li>An n-gram of size 1 is referred to as a “unigram”; size 2 is a “bigram” size 3 is a “trigram”. Larger sizes are sometimes referred to by the value of n in modern language, e.g., “four-gram”, “five-gram”, and so on.</li>
<li>Consider this example: <code>The quick brown fox jumped over the lazy dog.</code><ul>
<li>bigram would be <code>the quick</code>, <code>quick brown</code>, <code>brown fox</code>, …, i.e, every two consecutive words (or tokens)</li>
<li>trigram would be <code>the quick brown</code>, <code>quick brown fox</code>, <code>brown fox jumped</code>, …, i.e., every three consecutive words (or tokens)</li>
</ul>
</li>
<li>ngram model models sequence, i.e., predicts next word (n) given previous words (1, 2, 3, …, n-1)</li>
<li>multiple gram (bigram and above) captures <strong>context</strong></li>
<li>to choose n in n-gram requires experiments and making tradeoff between stability of the estimate against its appropriateness. Rule of thumb: trigram is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.</li>
<li>n-gram can be used as features for machine learning and downstream NLP tasks</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h3><ul>
<li>Why? Machine learning models cannot work with raw text directly; rather, they take numerical values as input.</li>
<li>Bag of words (BoW) builds a <strong>vocabulary</strong> of all the unique words in our dataset, and associate a unique index to each word in the vocabulary</li>
<li>It is called a “bag” of words, because it is a representation that completely ignores the order of words</li>
<li>Consider this example of two sentences: (1) <code>John likes to watch movies, especially horor movies.</code>, (2) <code>Mary likes movies too.</code> We would first build a vocabulary of unique words (all lower cases and ignoring punctuations): <code>[john, likes, to, watch, movies, especially, horor, mary, too]</code>. Then we can represent each sentence using term frequency, i.e, the number of times a term appears. So (1) would be <code>[1, 1, 1, 1, 2, 1, 1, 0, 0]</code>, and (2) would be <code>[0, 1, 0, 0, 1, 0, 0, 1, 1]</code></li>
<li>A common alternative to the use of dictionaries is the <a href="https://en.wikipedia.org/wiki/Feature_hashing" target="_blank" rel="noopener">hashing trick</a>, where words are directly mapped to indices with a hashing function</li>
<li>As the vocabulary grows bigger (tens of thousand), the vector to represent short sentences / document becomes sparse (almost all zeros)</li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><ul>
<li>Shallow, two-layer neural networks that are trained to construct linguistic context of words</li>
<li>Takes as input a large corpus, and produce a vector space, typically of several hundred<br>dimension, and each word in the corpus is assigned a vector in the space</li>
<li>The key idea is <strong>context</strong>: words that occur often in the same context should have same/opposite<br>meanings.</li>
<li>Two flavors<ul>
<li>continuous bag of words (CBOW): the model predicts the current word given a window of surrounding context words</li>
<li>skip gram: predicts the surrounding context words using the current word</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/w2v.png" alt="link"></p>
<p><a href="#data-science-question-answer">back to top</a></p>
<h2 id="System"><a href="#System" class="headerlink" title="System"></a>System</h2><ul>
<li><a href="#cron-job">Cron job</a></li>
<li><a href="#linux">Linux</a></li>
</ul>
<h3 id="Cron-job"><a href="#Cron-job" class="headerlink" title="Cron job"></a>Cron job</h3><p>The software utility <strong>cron</strong> is a <strong>time-based job scheduler</strong> in Unix-like computer operating systems. People who set up and maintain software environments use cron to schedule jobs (commands or shell scripts) to run periodically at fixed times, dates, or intervals. It typically automates system maintenance or administration – though its general-purpose nature makes it useful for things like downloading files from the Internet and downloading email at regular intervals.</p>
<p><img src="https://github.com/ShuaiW/data-science-question-answer/blob/master/assets/cron-job.PNG" alt="link"></p>
<p>Tools:</p>
<ul>
<li><a href="https://airflow.apache.org/" target="_blank" rel="noopener">Apache Airflow</a></li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p>Using <strong>Ubuntu</strong> as an example.</p>
<ul>
<li>Become root: <code>sudo su</code></li>
<li>Install package: <code>sudo apt-get install &lt;package&gt;</code></li>
</ul>
<p><a href="#data-science-question-answer">back to top</a></p>
<p>Confession: some images are adopted from the internet without proper credit. If you are the author and this would be an issue for you, please let me know.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-03-29-Study-AI-Code-Lv1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-03-29-Study-AI-Code-Lv1/" itemprop="url">AI Code Level 1</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-29T08:00:00+08:00">
                2020-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://github.com/Microsoft/ai-edu" target="_blank" rel="noopener">AI-Edu</a></p>
<h2 id="房价预测问题"><a href="#房价预测问题" class="headerlink" title="房价预测问题"></a>房价预测问题</h2><p>如果用传统的数学方法解决这个问题，我们可以使用正规方程，从而可以得到数学解析解，然后再使用神经网络方式来求得近似解，从而比较两者的精度，再进一步调试神经网络的参数，达到学习的目的。</p>
<p>我们不妨先把两种方式在这里做一个对比，读者阅读并运行代码，得到结果后，再回到这里来仔细体会表5-3中的比较项。</p>
<p>表5-3 两种方法的比较</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>正规方程</th>
<th>梯度下降</th>
</tr>
</thead>
<tbody>
<tr>
<td>原理</td>
<td>几次矩阵运算</td>
<td>多次迭代</td>
</tr>
<tr>
<td>特殊要求</td>
<td>$X^TX$的逆矩阵存在</td>
<td>需要确定学习率</td>
</tr>
<tr>
<td>复杂度</td>
<td>$O(n^3)$</td>
<td>$O(n^2)$</td>
</tr>
<tr>
<td>适用样本数</td>
<td>$m \lt 10000$</td>
<td>$m \ge 10000$</td>
</tr>
</tbody>
</table>
<p>DialogFlow，Python 和 Flask 打造 ChatBot<br><a href="https://github.com/virgili0/Virgilio/blob/master/zh-CN/Topics/DialogFlow.md" target="_blank" rel="noopener">https://github.com/virgili0/Virgilio/blob/master/zh-CN/Topics/DialogFlow.md</a></p>
<p>Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology<br><a href="https://github.com/virgili0/Virgilio/blob/master/zh-CN/Topics/Computer%20Vision/Object_Instance_Segmentation_using_TensorFlow_Framework_and_Cloud_GPU_Technology.ipynb" target="_blank" rel="noopener">https://github.com/virgili0/Virgilio/blob/master/zh-CN/Topics/Computer%20Vision/Object_Instance_Segmentation_using_TensorFlow_Framework_and_Cloud_GPU_Technology.ipynb</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-03-28-Study-Data-Science-for-Business-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-03-28-Study-Data-Science-for-Business-Notes/" itemprop="url">Data Science for Business Notes</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-28T08:00:00+08:00">
                2020-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Foster Provost and Tom Fawcett, Data Science for Business: What you need to know about data mining and data-analytic thinking, O’Reilly Media; 1 edition (August 16, 2013)</p>
<ol>
<li>Introduction: Data-Analytic Thinking<ol>
<li>Data science: a set of fundamental principles that guide the extraction of knowledge from data</li>
<li>Churn: customers switching from one company to another</li>
<li>DDD: Data-Driven Decition Making =by&gt; Data science involces principles, processes, and techniques for understanding phenomena via the (automated) analysis of data. <a href="0">data-driven-decision-making-notes.pdf</a><ol>
<li>Discovery</li>
<li>Accuracy</li>
</ol>
</li>
<li>Big Data: datasets that are too large for traditional data processing systems</li>
<li>Data, Data Science Capability as a Dtrategic Asset: the capability to extract useful knowledge from data</li>
<li>Fundamental concept:<ol>
<li>Extracting useful knowledge from data to solve business problemscan be treated systematically by following a process with reasonably well-defined stages</li>
<li>From a large mass of data, information technology can be used tofind  informative  descriptive  attributes of entities of interest</li>
<li>If you look too hard at a set of data, you will find something—butit might not generalize beyond the data you’re looking at</li>
<li>Formulating  data  mining  solutions  and  evaluating  the  resultsinvolves thinking carefully about the context in which they will be used</li>
</ol>
</li>
</ol>
</li>
<li>Business Problems and Data Science Solutions<ol>
<li>Fundamentally different types of tasks data mining algorithms address:<ol>
<li>Classification and class probability estimation<ol>
<li>attempts to predict, for each individual in a population, which of a (small) set of classes this individual belongs to (binary target)</li>
<li>scoring model applied to an individual produces a score representing the probability that that individual belongs to each class</li>
</ol>
</li>
<li>Regression (“value estimation”)<ol>
<li>attempts to estimate or predict, for each individual, the numerical value of some variable for that individual (numeric target)</li>
</ol>
</li>
<li>Similarity matching<ol>
<li>attempts to identify similar individuals based on data known about them</li>
</ol>
</li>
<li>Clustering<ol>
<li>attempts to group individuals in a population together by their similarity, but not driven by any specific purpose</li>
</ol>
</li>
<li>Co-occurrence grouping<ol>
<li>attempts to find associations between entities based on transactions involving them</li>
</ol>
</li>
<li>Profiling (behaviour description)<ol>
<li>attempts to characterize the typical behaviour of an individual, group, or population</li>
</ol>
</li>
<li>Link prediction<ol>
<li>attempts to predict connections between data items, usually by suggesting that a link should exist, and possibly also estimating the strength of the link</li>
</ol>
</li>
<li>Data reduction<ol>
<li>attempts to take a large set of data and replace it with a smaller set of data that contains much of the important information in the larger set</li>
</ol>
</li>
<li>Casual modelling<ol>
<li>attempts to understand what events or actions actually influence others</li>
</ol>
</li>
</ol>
</li>
<li>Unsupervised: data mining problem when there is no clear target</li>
<li>Supervised: data mining problem where a specific target is defined and there must be data on the target</li>
<li>Label: the value for the target variable for an individual</li>
<li>Data mining algorithms:<ol>
<li>supervised methods<ol>
<li>classification</li>
<li>regression</li>
<li>casual modelling</li>
<li>similarity matching</li>
<li>link prediction</li>
<li>data reduction</li>
</ol>
</li>
<li>unsupervised<ol>
<li>clustering</li>
<li>co-occurrence grouping</li>
<li>profiling</li>
<li>similarity matching</li>
<li>link prediction</li>
<li>data reduction</li>
</ol>
</li>
</ol>
</li>
<li>The data mining process given by the Cross Industry Standard Process for Data Mining (CRSIP-DM):<ol>
<li>Business Understanding<ul>
<li>to understand the problem to be solved</li>
<li>the design team should think carefully about the problem to be solved and about the use scenario (the analysts’ creativity plays a large role)</li>
</ul>
</li>
<li>Data Understanding<ul>
<li>to understand the strengths and limitations of the data because there rarely is there an exact match with the problem</li>
<li>cost of data may vary (some data may be purchased, free, or doesn’t exist at all)</li>
</ul>
</li>
<li>Data Preparation<ul>
<li>conversing the data into forms that yield better results (removing or inferring missing values, converting data to different types)</li>
<li>leakage must be considered carefully (a situation where a variable collected in historical data gives information on the target variable – information that appears in historical data but is not actually available when the decision has to be made)</li>
</ul>
</li>
<li>Modelling<ul>
<li>the output of modelling is some sort of model or pattern capturing regularities in the data</li>
</ul>
</li>
<li>Evaluation<ul>
<li>to assess the data mining results rigorously and to gain confidence that they are valid and reliable before moving on</li>
<li>ensure that the model satisfies the original business goals</li>
<li>evaluating the results includes both quantitative and qualitative assessments</li>
</ul>
</li>
<li>Deployment<ul>
<li>the results are put into real use in order to realize some return on investment</li>
</ul>
</li>
</ol>
</li>
<li>Related analytic techniques:<ol>
<li>Statistics</li>
<li>Database Querying</li>
<li>Data Warehousing</li>
<li>Regression Analysis</li>
<li>Machine Learning and Data Mining</li>
</ol>
</li>
</ol>
</li>
<li>Introduction to Predictive Modelling: From Correlation to Supervised Segmentation<ol>
<li>Model: a simplified representation of reality created to serve a purpose</li>
<li>Predictive modelling: the primary purpose of the model is to estimate an unknown value</li>
<li>Descriptive modelling: the primary purpose of the model is to gain insight into the underlying phenomenon or process</li>
<li>Supervised learning: model creation where the model describes a relationship between a set of selected variables (attributes or features) and a predefined variable called the target variable</li>
<li>Instance (feature vector): a set of attributes (fields, columns, variables, or features)</li>
<li>Model induction: the creation of models</li>
<li>Induction algorithm/learner: the procedure that creates the model from the data</li>
<li>Training data/labelled data: the input data for the induction algorithm, used for inducing the model</li>
<li>Supervised Segmentation &amp; Selecting Informative Attributes - Pure: homogeneous with respect to the target variable</li>
<li>Entropy: a measure of disorder that can be applied to a set</li>
<li>Information Gain (IG): measures the change in entropy due to any amount of new information being added</li>
<li>Classification tree are one sort of tree-structured model</li>
<li>probability estimation tree - Y/N</li>
<li>regression tree - numeric</li>
</ol>
</li>
<li>Fitting a Model to Data<ol>
<li>Parameter learning/parametric modeling: tuning parameters so that the model fits the data as well modelling as possible</li>
<li>Linear discriminant: linear functions (𝑦 = 𝑚𝑥 + 𝑏) that discriminates between the classes, and the function of the decision boundary is a linear combination – a weighted sum – of the attributes</li>
<li>Support vector machine (SVM)<ol>
<li>First fit the fattest bar between the classes</li>
<li>the linear discriminant will be the center line through the dashed parallel lines (margin)</li>
<li>has hinge loss - “small” errors</li>
</ol>
</li>
<li>Regression via Mathematical Functions<ol>
<li>Loss functions - hinge loss, Zero-one loss</li>
<li>absolute error</li>
</ol>
</li>
<li>Least squares regression: its very sensitive to the date, outlying data points can severely skew the resultant linear function</li>
<li>Logistic regression f(x) is the model’s estimation of the log-odds that x belongs to the positive class<ol>
<li>Most important parts logistic regression</li>
<li>Differences between classification trees and linear classifiers</li>
</ol>
</li>
<li>Neural networks: can be seen as a “stack” of models</li>
</ol>
</li>
<li>Overfitting and Its Avoidance<ol>
<li>Generalization - the property of a model or modelling process, whereby the model applies to data that were not used to build the model</li>
<li>Overfitting - the tendency of data mining procedures to tailor models to the training data, at the expense of generalization to previously unseen data points<ol>
<li>There is a fundamental trade-off between model complexity and the possibility of overfitting.</li>
</ol>
</li>
<li>Fitting graph - shows the accuracy of a model as a function of complexity<ol>
<li>Estimate generalization performance by comparing the predicted values with the hidden true values.</li>
</ol>
</li>
<li>Holdout data - data not used in building the model, but for which we do know the actual value of the target variable<ol>
<li>Generally, there will be more overfitting as one allows the model to be more complex.</li>
<li>One way mathematical functions can become more complex is by adding more variables (more attributes) or changing the function from being truly linear to nonlinear.</li>
<li>Overfitting is bad because as a model gets more complex it is allowed to pick up harmful spurious correlations. The harm occurs when these spurious correlations produce incorrect generalizations in the model.</li>
<li>A holdout set gives us an estimate of generalization performance.</li>
</ol>
</li>
<li>Cross-validation - a more sophisticated holdout training and testing procedure<ol>
<li>Cross-validation begins by splitting a labelled dataset into k partitions called folds. The purpose of cross-validation is to use the original labelled data efficiently to estimate the performance of a modelling procedure. The result is k-different accuracy results, which then can be used to compute the average accuracy and its variance.</li>
</ol>
</li>
<li>Learning curve - a plot of the generalization performance against the amount of training data<ol>
<li>A learning curve shows the generalization performance – the performance only on testing data, plotted against the amount of training data used. A fitting graph shows the generalization performance as well as the performance on the training data, but plotted against model complexity.</li>
<li>Tree induction uses two techniques to avoid overfitting:<ol>
<li>stop growing the tree before it gets too complex<ol>
<li>the threshold can be determined using a hypothesis test.</li>
</ol>
</li>
<li>grow the tree until it is too large, then “prune” it back, reducing its size (and thereby its complexity)</li>
</ol>
</li>
<li>The general method for avoiding overfitting is to choose the value for some complexity parameter by using some sort of nested holdout procedure.</li>
</ol>
</li>
<li>Sequential forward selection (SFS) - uses a nested holdout procedure to first pick the best individual feature, by looking at all models built using just one feature</li>
<li>Regularization - a combination of fit to the data and simplicity of the model</li>
</ol>
</li>
<li>Similarity, Neighbors, and Clusters<ol>
<li>Similarity and Distance (Nearest-Neighbor Reasoning)<ol>
<li>nearest-neighbor methods often use weighted  voting or imilarity moderated voting</li>
<li>instance-based learning</li>
<li>Case-Based Reasoning</li>
</ol>
</li>
<li>Geometric Interpretation, Overfitting, and Complexity Control<ol>
<li>k-NN e.g. k = n | k = 1</li>
<li>Issues with Nearest-Neighbor Methods<ol>
<li>Intelligibility</li>
<li>Dimensionality and domain knowledge</li>
</ol>
</li>
</ol>
</li>
<li>Technical Details - Calculating Scores from Neighbors<ol>
<li>Heterogeneous Attributes</li>
<li>Manhattan distance or L1-norm</li>
<li>Jaccard distance</li>
<li>Cosine distance - text classification</li>
<li>edit distance or the Levenshtein metric</li>
<li>Majority vote classification</li>
<li>Majority scoring function</li>
<li>Similarity-moderated classification</li>
<li>Similarity-moderated scoring</li>
<li>Similarity-moderated regression</li>
</ol>
</li>
<li>Clustering<ol>
<li>Hierarchical Clustering - e.g. Tree of Life</li>
<li>Clustering Around Centroids - k-means clustering</li>
<li>Using Supervised Learning to Generate Cluster Descriptions</li>
<li>CRISP data mining process</li>
</ol>
</li>
</ol>
</li>
<li>Decision Analytic Thinking I: What Is a Good Model?<ol>
<li>Evaluating Classifiers<ol>
<li>Classifier accuracy</li>
<li>Confusion Matrix</li>
</ol>
</li>
<li>Generalizing Beyond Classification</li>
<li>A Key Analytical Framework: Expected Value<ol>
<li>cost-benefit matrix</li>
<li>b(predicted, actual)</li>
</ol>
</li>
<li>Evaluation, Baseline Performance, and Implications for Investments in Data</li>
</ol>
</li>
<li>Visualizing Model Performance<ol>
<li>Profit Curves<ol>
<li>class priors</li>
<li>costs and benefits</li>
</ol>
</li>
<li>Receiver Operating Characteristics (ROC) Graphs and Curves</li>
<li>Cumulative Response Curves (Lift Curves)</li>
<li>Evaluating Learning Algorithms: A Classification Perspective (Japkowicz &amp; Shah, 2011)</li>
</ol>
</li>
<li>Evidence and Probabilities<ol>
<li>Combining Evidence Probabilistically<ol>
<li>Joint Probability and Independence</li>
<li>Bayes’ Rule - <a href="https://www.cnblogs.com/pegasus923/p/10470230.html" target="_blank" rel="noopener">Notes</a></li>
</ol>
</li>
<li>Applying Bayes’ Rule to Data Science<ol>
<li>Conditional Independence and Naive Bayes</li>
<li>Advantages and Disadvantages of Naive Bayes</li>
</ol>
</li>
<li>A Model of Evidence “Lift”</li>
<li>Example: Evidence Lifts from Facebook “Likes”</li>
</ol>
</li>
<li>Representing and Mining Text<ol>
<li>Representation<ol>
<li>Bag of Words</li>
<li>Term Frequency</li>
<li>Inverse document frequency (IDF) &amp; TFIDF</li>
</ol>
</li>
<li>The Relationship of IDF to Entropy ?</li>
<li>Beyond Bag of Words<ol>
<li>N-gram Sequences</li>
<li>Named Entity Extraction</li>
</ol>
</li>
</ol>
</li>
<li>Decision Analytic Thinking II: Toward Analytical Engineering<ol>
<li>The Expected Value Framework: Decomposing the Business Problem and Recomposing the Solution Pieces</li>
<li>The Expected Value Framework: Structuring a More Complicated Business Problem<ol>
<li>value of targeting</li>
<li>From an Expected Value Decomposition to a Data Science Solution</li>
</ol>
</li>
</ol>
</li>
<li>Other Data Science Tasks and Techniques<ol>
<li>Co-occurrences and Associations: Finding Items That Go Together</li>
<li>Profiling: Finding Typical Behavior<ol>
<li>Gaussian Mixture Model (GMM)</li>
</ol>
</li>
<li>Link Prediction and Social Recommendation<ol>
<li>multivariate similarity</li>
</ol>
</li>
<li>Data Reduction, Latent Information, and Movie Recommendation<ol>
<li>regularization</li>
</ol>
</li>
<li>Bias, Variance, and Ensemble Methods<ol>
<li>k-NN prediction</li>
<li>inherent randomness, learning curves</li>
</ol>
</li>
<li>Data-Driven Causal Explanation and a Viral Marketing Example<ol>
<li>Causal explanation</li>
<li>Causal data analysis</li>
</ol>
</li>
</ol>
</li>
<li>Data Science and Business Strategy<ol>
<li>Thinking Data-Analytically, Redux</li>
<li>Achieving Competitive Advantage with Data Science</li>
<li>Sustaining Competitive Advantage with Data Science</li>
<li>Attracting and Nurturing Data Scientists and Their Teams</li>
<li>Be Ready to Accept Creative Ideas from Any Source</li>
<li>Be Ready to Evaluate Proposals for Data Science Projects</li>
<li>A Firm’s Data Science Maturity</li>
</ol>
</li>
<li>Conclusion<ol>
<li>The Fundamental Concepts of Data Science</li>
<li>“What Data Can’t Do” (Brooks, 2013) New York Times</li>
<li>Privacy, Ethics, and Mining Data About Individuals</li>
<li>ACM SIGKDD International Conference on Data Mining and Knowledge Discovery</li>
<li>IEEE International Conference on Data Mining</li>
<li>Final Example: From Crowd-Sourcing to Cloud-Sourcing<ol>
<li>Amazon Mechanical Turk (MTurk)</li>
</ol>
</li>
</ol>
</li>
</ol>
<ul>
<li>Understanding Data and the Data Mining Process</li>
<li>Exploring and Visualizing Data</li>
<li>Market Segmentation of Customers</li>
<li>Working with Unstructured Datasets</li>
<li>Predictive Modeling</li>
<li>Overfitting and Evaluating Models</li>
<li>Data Mining Techniques for Prediction</li>
<li>Pro-Active Churn Management</li>
<li>Data Based Decision Making</li>
<li>Business Strategy for Employing Data Science</li>
</ul>
<p>Tools:</p>
<ol>
<li>Calculus</li>
<li>Linear Algebra - <a href="https://mitmath.github.io/1806/" target="_blank" rel="noopener">MIT Math</a></li>
<li>Probability And Statitics</li>
<li>Statistical Inference</li>
<li>Applied Linear Regression 4th edition by Weisberg</li>
<li>Statistics for Experimenters: Design, Innovation, and Discovery, 2nd Edition</li>
<li>Probability and Measure</li>
<li>SVD cholesky decomposition matrix differentiation</li>
<li>..</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-03-26-Study-Data-Science-Links/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-03-26-Study-Data-Science-Links/" itemprop="url">Data Science Links</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-26T08:00:00+08:00">
                2020-03-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="数据科学家修炼之道"><a href="#数据科学家修炼之道" class="headerlink" title="数据科学家修炼之道"></a>数据科学家修炼之道</h2><p>[美] Zacharias Voulgaris 弗格里斯</p>
<h3 id="有用的网页链接"><a href="#有用的网页链接" class="headerlink" title="有用的网页链接"></a>有用的网页链接</h3><ul>
<li><a href="http://www.kaggle.com" target="_blank" rel="noopener">www.kaggle.com</a> - 人际网络、大数据分析竞赛、找工作可能是目前有关于数据科学家和机器学习专家的最流行的网站。一定要记得把它添加进入你的浏览器收藏夹</li>
<li><a href="http://www.linkedin.com" target="_blank" rel="noopener">www.linkedin.com</a> - 人际网络、找工作、在线简历最流行的社交平台之一，适合于各种工作各种级别的人加入。有很多领域内小组，并且可以在上面遇到其他的数据科学家并向他们学习</li>
<li><a href="http://www.datasciencecentral.com" target="_blank" rel="noopener">www.datasciencecentral.com</a> - 文章、找工作、人际网络、新闻、背景知识也是最流行的一个高质量数据科学网站。对于数据科学专家和新手都很适合，是一个非常好的寻找灵感和技术的地方</li>
<li><a href="http://www.coursera.com" target="_blank" rel="noopener">www.coursera.com</a> - 在线学习、人际网络是目前最流行的在线教育平台，由斯坦福大学的一群教授建立。上边有一些有关于数据科学的课程，在每一个课程的论坛中，你可以找到一些其他志同道合的人</li>
<li><a href="http://datascience01.wordpress.com" target="_blank" rel="noopener">datascience01.wordpress.com</a> - 新闻一个适合新人找到最新领域内新闻的地方</li>
<li><a href="http://cran.rproject.org" target="_blank" rel="noopener">cran.rproject.org</a> - R、新闻、软件一个熟悉R语言开发流程的好地方，并且可以轻松下载各种R包进行开发</li>
<li><a href="http://www.indeed.com" target="_blank" rel="noopener">www.indeed.com</a> - 找工作、在线简历最著名的招聘网站</li>
<li><a href="http://whatsthebigdata.com" target="_blank" rel="noopener">whatsthebigdata.com</a> - 新闻、背景知识、文章、大数据一个可以很好地拓展自己的大数据领域知识，并且可以发现最多目前的前沿技术发展情况的地方</li>
<li><a href="http://www.bigdatauniversity.com" target="_blank" rel="noopener">www.bigdatauniversity.com</a> - 在线学习、大数据最好的获得数据技术专业知识和资源的地方，由IBM创建与维护</li>
<li><a href="http://www.rproject.org" target="_blank" rel="noopener">www.rproject.org</a> - R、新闻、软件适合开发者经常浏览的，充满了大量流行的大数据分析方法的开源平台</li>
<li><a href="http://www.eclipse.org" target="_blank" rel="noopener">www.eclipse.org</a> - Eclipse、新闻、软件最流行的一个面向对象编程语言集成开发平台</li>
<li><a href="http://Hadoop.apache.org" target="_blank" rel="noopener">Hadoop.apache.org</a> - 大数据、Hadoop软件最流行的大数据技术平台</li>
<li><a href="http://www.careerealism.com" target="_blank" rel="noopener">www.careerealism.com</a> - 找工作一个充满了很多找工作教程和指导资源的网站</li>
<li><a href="http://stackoverflow.com" target="_blank" rel="noopener">stackoverflow.com</a> - 技术问题一个可以找到各种技术问题的解决方案的地方（例如R语言）</li>
<li><a href="http://coursetalk.org" target="_blank" rel="noopener">coursetalk.org</a> - 在线学习一个适合学生们找到各种各样的课程进行学习的在线教育平台，并且学生们可以评价他们选择过的课程</li>
<li><a href="http://www.edX.org" target="_blank" rel="noopener">www.edX.org</a> - 在线学习一个可以提供不同课题的大数据课程的在线教育平台</li>
<li><a href="http://www.classcentral.com" target="_blank" rel="noopener">www.classcentral.com</a> - 在线学习一个可以从不同的在线教育平台找到各种课程资源的网站，例如Coursera、edX等</li>
<li><a href="http://www.technicspub.com" target="_blank" rel="noopener">www.technicspub.com</a> - 线下学习一个可以找到各种技术书籍的网站</li>
<li><a href="http://www.java.com" target="_blank" rel="noopener">www.java.com</a> - 面向对象编程、Java、软件目前世界上最流行的面向对象编程语言之一</li>
<li><a href="http://www.python.org" target="_blank" rel="noopener">www.python.org</a> - 面向对象编程、Python、Software目前世界上最流行的面向对象编程语言之一</li>
<li><a href="http://www.gnu.org/software/octave" target="_blank" rel="noopener">www.gnu.org/software/octave</a> - 面向对象编程、Octave、软件目前最流行的一个开源数据分析平台之一，与Matlab非常类似</li>
<li><a href="http://www.mathworks.com/matlabcentral" target="_blank" rel="noopener">www.mathworks.com/matlabcentral</a> - 面向对象编程、Matlab、软件目前非常流行的一个数据分析平台</li>
<li><a href="http://www.tableausoftware.com" target="_blank" rel="noopener">www.tableausoftware.com</a> - 数据可视化、软件目前最流行的一种可视化软件</li>
<li><a href="http://www.1.ibm.com/software/data/infosphere/biginsights" target="_blank" rel="noopener">ibm infosphere biginsights</a> - 大数据、软件IBM开发的一个非常适于处理大数据的平台（基于Hadoop）</li>
<li><a href="http://gitscm.com" target="_blank" rel="noopener">gitscm.com</a> - 版本控制、软件一个非常流行的用来控制软件或者程序版本的网站</li>
<li><a href="http://www.oracle.com" target="_blank" rel="noopener">www.oracle.com</a> - 数据库管理、软件目前世界上最流行的一个数据库</li>
<li><a href="http://www.datascience201.com?kid=21QD6" target="_blank" rel="noopener">www.datascience201.com</a> - 在线学习一个可以找到很多学习数据科学的各种有趣资源的地方，主要针对数据新手</li>
<li><a href="http://www.udacity.com" target="_blank" rel="noopener">www.udacity.com</a> - 在线学习一个可以提供大量数据科学课题学习的在线学习平台</li>
<li><a href="http://www.cs.toronto.edu/~hinton" target="_blank" rel="noopener">www.cs.toronto.edu</a> - 个人目前世界上最著名的一个数据科学家的主页</li>
<li><a href="http://www.meetup.com" target="_blank" rel="noopener">www.meetup.com</a> - 人际网络、线下学习目前最好的一个基于线下自组织团体的社交平台。上面有很多专业的团队，非常适合用来建立人际网络以及寻找教育资源</li>
</ul>
<h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><ul>
<li><a href="flowingdata.com/">Link</a>数据科学家的职责有关于过往几年的数据科学的流行情况、相关技术以及其他有关于数据科学家的信息的文章</li>
<li><a href="www.datacommunitydc.org/">Link</a>数据产品有关于最近的流行的数据产品的文章</li>
<li><a href="www.cooldailyinfographics.com/post/describinghowdifferentindustrieshavecapitalizedbigdata">Link</a>信息图、大数据有关于数据科学在各个领域的功能与作用的信息图谱</li>
<li><a href="gigaom.com/2011/09/30/bigdataequalsbigopportunitiesforbusinessesinfographic">Link</a>信息图、大数据另一个有关于大数据在不同领域中发挥作用的信息图谱</li>
<li><a href="whatsthebigdata.com/">Link</a>数据科学史提供长年累月依赖的经典数据科学里程碑信息的文章</li>
<li><a href="www.forbes.com/sites/gilpress/2013/05/28/averyshorthistoryofdatascience">Link</a>数据科学史另一篇有关于数据科学史的文章</li>
<li><a href="www.verious.com/tutorial/bigdatastoragemediumsdatastructures">Link</a>技术简介、大数据非常有用的有关于数据存储和大数据数据库的文章</li>
<li><a href="howtojboss.com/2013/02/13/bigdatastoragemediumsdatastructures">Link</a>技术简介、大数据有关于数据存储与数据库的重要文章</li>
<li><a href="www.techopedia.com/definition/28789/dataexploration">Link</a>数据科学流程、数据探索有关于数据探索以及其在数据科学中的作用的文章</li>
<li><a href="www.bytemining.com/2011/08/HadoopfatiguealternativestoHadoop">Link</a>技术简介、大数据技术介绍除了Hadoop以外的其他大数据处理平台的文章</li>
<li><a href="www.enterpriseappstoday.com/datamanagement/4hotopensourcebigdataprojects.html">Link</a>技术简介、大数据科技另一篇有关于Hadoop替代品的文章</li>
<li><a href="strata.oreilly.com/2012/02/whatisapacheHadoop.html">Link</a>技术简介、大数据技术 Hadoop平台系统简介</li>
<li><a href="apandre.wordpress.com/tools/comparison/">Link</a>数据科学流程、数据可视化有关于比较当前的不同的数据可视化工具的文章</li>
<li><a href="www.mlplatform.nl/whatismachinelearning">Link</a>数据科学史、机器学习有关于机器学习经典算法的简要介绍</li>
<li><a href="sge.wonderville.ca/machinelearning/history/history.html">Link</a>数据科学史、机器学习更多有关于机器学习的介绍，包括其经典算法</li>
<li><a href="cran.rproject.org/web/views/MachineLearning.html">Link</a>机器学习、R包有关于机器学习包和库的完整列表</li>
<li><a href="www.datasciencecentral.com/profiles/blogs/datascientistcoreskills">Link</a>数据科学家技能有关于当前的数据科学家所需技能的文章</li>
<li><a href="datainformed.com/glossaryofbigdataterms">Link</a>大数据、词汇表本书词汇表来源之一</li>
<li><a href="www.bigdatastartups.com/abcbigdataglossaryterminology">Link</a>大数据、词汇表另一个本书的词汇表来源</li>
<li><a href="www.mhhe.com/business/buscom/gregg/docs/appd.pdf">Link</a>词汇表</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://blog.toob.net.cn/2020-03-18-Study-Digital-forensics-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Xia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Xia">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020-03-18-Study-Digital-forensics-Notes/" itemprop="url">Digital forensics Notes</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T08:00:00+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Prescribed textbook: André Årnes, Digital Forensic, Wiley; 1 edition (July 24, 2017)</p>
<p>Session 1: Introduction of Digital Forensics</p>
<ol>
<li>Introduction - Andre Arnes<ol>
<li>History</li>
<li>Locard’s Exchange Principle</li>
<li>Crime Reconstruction</li>
<li>Ivestigations</li>
<li>Evidence Dynamics</li>
<li>Layers of Abstraction</li>
<li>Metadata</li>
<li>Online Bank Fraud<ol>
<li>Modus Operandi</li>
<li>The SpyEye Case</li>
</ol>
</li>
</ol>
</li>
<li>The Digital Forensics Process - Anders O. Flaglien<ol>
<li>Principles of a Forensics Process</li>
<li>Finding the Digital Evidence</li>
<li>The Identification Phase<ol>
<li>Preparations and Deployment of Tools and Resources</li>
<li>The First Responder</li>
<li>At the Scene of the Incident</li>
<li>Dealing with Live and Dead Systems</li>
<li>Chain of Custody</li>
</ol>
</li>
<li>The Collection Phase<ol>
<li>Sources of Digital Evidence</li>
<li>Systems Physically Tied to a Location</li>
<li>Multiple Evidence Sources</li>
<li>Reconstruction</li>
<li>Evidence Integrity</li>
<li>Dual-Tool Verification</li>
<li>Remote Acquisition</li>
<li>External Competency and Forensics Cooperation</li>
</ol>
</li>
<li>The Examination Phase<ol>
<li>Initial Data Source Examination and Preprocessing</li>
<li>Forensic File Formats and Structures</li>
<li>Data Reduction and Filtering</li>
<li>Timestamps</li>
<li>Compression, Encryption and Obfuscation</li>
<li>Data and File Carving</li>
<li>Automation</li>
</ol>
</li>
<li>The Analysis Phase<ol>
<li>Layers of Abstraction</li>
<li>Evidence Types</li>
<li>String and Keyword Searches</li>
<li>Anti-Forensics<ol>
<li>Computer Media Wiping</li>
<li>Analysis of Encrypted and Obfuscated Data</li>
</ol>
</li>
<li>Automated Analysis</li>
<li>Timelining of Events</li>
<li>Graphs and Visual Representaitons</li>
<li>Link Analysis</li>
</ol>
</li>
<li>The Presentation Phase<ol>
<li>The Final Reports</li>
<li>Presentation of Evidence and Work Conducted</li>
<li>The Chain of Custody Circle Closes</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>Session 2-4: Traditional Digital Forensics Investigation</p>
<ol>
<li>Cybercrime Law - Inger Marie Sunde<ol>
<li>The International Legal Framework of Cybercrime Law<ol>
<li>The International Involved in Criminal Activity and in Crime-Prevention Initiatives</li>
<li>The National Legal System Versus the International legal Framework</li>
<li>Fundamental Rights Relating to Cybercrime Law - The ECHR<ol>
<li>The ECtHR as a Driving Force for Development of Human Rights</li>
<li>The Right to Bring a Case before the ECtHR</li>
<li>A Special Note on Transborder Search and the rule of Law</li>
<li>The Connection between Fundamental Rights and the Rule of Law</li>
<li>The Principle of Legality in the Context of Crime</li>
<li>The Principle of Legality in the Context of a Criminal Investigation</li>
<li>The Positive Obligation of the Nation State</li>
<li>The Right to Fair Trial</li>
<li>A Special Note on Evidence Rules in Different Legal Systems</li>
<li>Possible Outcomes of a Violation of Fundamental Rights</li>
</ol>
</li>
<li>Special Legal Framework: The Cybercrime Convention</li>
<li>Interpretation of Cybercrime Law<ol>
<li>Interpretation of Substantive Criminal Law</li>
<li>Application of Old Criminal Provisions to New Modes of Conduct</li>
<li>Interpretation of Procedural Procedural Provisions Authorizing Coersive Measures</li>
</ol>
</li>
</ol>
</li>
<li>Digital Crime - Substantive Criminal Law<ol>
<li>General Conditions for Criminal Liability</li>
<li>Real-Life Modus Operandi</li>
<li>Offenses against the Confidentiality, Integrity, and Availability of Computer Data and Systems<ol>
<li>Illegal Access and Illegal Interception</li>
<li>Data and System Interference</li>
<li>Misuse of Devices</li>
</ol>
</li>
<li>Computer-Related Offenses</li>
<li>Content-Related Offenses</li>
<li>Offenses Related to Infringements of Copyright and Related Rights</li>
<li>Racist and Xenophobic Speech</li>
</ol>
</li>
<li>Investigation Methods for Collecting Digital Evidence<ol>
<li>The Digital Forensic Process in the Context of Criminal Procedure</li>
<li>Computer Data That Are Publicly Available<ol>
<li>Transborder Access to Stored Computer Data Where Publicly Available</li>
<li>Online Undercover Operations</li>
</ol>
</li>
<li>Scope and Safeguards fo the Investigation Methods<ol>
<li>Suspicion-Based Investigation Methods</li>
<li>The Scope of the Investigation Methods</li>
<li>Conditions and Safeguards</li>
<li>Considerations Relating to Third Parties</li>
</ol>
</li>
<li>Search and Seizure<ol>
<li>Main Rules</li>
<li>Special Issues</li>
</ol>
</li>
<li>Production Order</li>
<li>Expedited Preservation and Partial Disclosure of Traffic Data<ol>
<li>Real-Time Investigation Methods</li>
</ol>
</li>
</ol>
</li>
<li>International Cooperation in Order to Collect Digital Evidence<ol>
<li>Narowing the Focus</li>
<li>A Special Note on Transborder Access to Digital Evidence</li>
<li>Mutual Legal Assistance<ol>
<li>Basic Principles and Formal steps of the procedure</li>
<li>International Police Cooperation and Joint Investigation</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Digital Forensic Readiness - Ausra Dilijonaite<ol>
<li>Definition</li>
<li>Law Enforcement versus Enterprise Digital Forensic Readiness</li>
<li>Why? A Rationale for Difital Forensic Readiness<ol>
<li>Cost</li>
<li>Usefulness of Digital Evidence<ol>
<li>Existence of Digital Evidence</li>
<li>Evidentiary Weight of Digital Evidence</li>
</ol>
</li>
</ol>
</li>
<li>Frameworks, Standards, and Methodologies<ol>
<li>Standards<ol>
<li>ISO/IEC 27037</li>
<li>ISO/IEC 17025</li>
<li>NIST SP 800-86</li>
</ol>
</li>
<li>Guidelines<ol>
<li>IOCE Guidelines</li>
<li>Scientific Working Group on Digital Evidence (SWGDE)</li>
<li>ENFSI Guidelines</li>
</ol>
</li>
<li>Research<ol>
<li>Rowlingson’s Ten-step Process</li>
<li>Grobler et al.’s Forensic Readiness Framework</li>
<li>Endicott-Popovsky et al.’s Forensic Readiness Framework</li>
</ol>
</li>
</ol>
</li>
<li>Becoming “Digital Forensic” Ready</li>
<li>Enterprise Digital Forensic Readiness<ol>
<li>Legal Aspects</li>
<li>Policy, Processes, and Procedures</li>
<li>People</li>
<li>Technology: Digital Forensic Laboratory</li>
<li>Technology: Tools and Infrastructure</li>
<li>Outsourcing Digital Forensic Capabilities</li>
</ol>
</li>
<li>Considerations for Law Enforcement</li>
</ol>
</li>
<li>Computer Forensics - Jeff Hamm<ol>
<li>Evidence Collection<ol>
<li>Data Acquisition<ol>
<li>Live Data</li>
<li>Forensic</li>
</ol>
</li>
<li>Forensic Copy</li>
</ol>
</li>
<li>Examination<ol>
<li>Disk Structures</li>
<li>File Systems</li>
</ol>
</li>
<li>Analysis<ol>
<li>Analysis Tools</li>
<li>Timeline Analysis</li>
<li>File Hashing</li>
<li>Filtering</li>
<li>Data Carving</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>Session 5: Network Forensics Investigation<br>Internet Forensics</p>
<ol>
<li></li>
<li></li>
<li></li>
<li></li>
<li>Tracing Information on the Internet<ol>
<li>DNS and Reverse DNS</li>
<li>Whois and Reverse Whois</li>
<li>Ping and Port Scan</li>
<li>Traceroute</li>
<li>IP Geolocation</li>
<li>Tracing BitTorrent Peers</li>
<li>Bitcoin Unconfirmed Transaction Tracing</li>
</ol>
</li>
<li>Collection Phase - Local Acquisition<ol>
<li>Server: Server Logs, Web Application Logs, Virtual Hosts</li>
<li>Cloud Services</li>
<li>Open Sources</li>
</ol>
</li>
<li>Other Considerations<ol>
<li>APIs</li>
<li>Integrity of Remote Artifacts</li>
</ol>
</li>
<li>The Examination and Analysis Phases</li>
</ol>
<p>Session 6: Live and Memory Forensics Investigation</p>
<p>Session 7: Mobile Forensics Investigation<br>Mobile and Embedded Forensics - Jens-Petter Sandvik</p>
<ol>
<li>Introduction<ol>
<li>Embedded Systems and Consumer Electronics</li>
<li>Mobile Phone</li>
<li>Telecommunication Networks</li>
<li>Mobile Devices and Embedded Systems as Evidence</li>
<li>Malware and Security Considerations</li>
</ol>
</li>
</ol>
<p>Session 8-9: Practical Exercise<br>Educational Guide - Stefan Axelsson</p>
<p>Session 10: Big Data Forensics<br>Challenges in Digital Forensics - Katrin Franke and Andre Arnes</p>
<ol>
<li>Computational Forensics</li>
<li>Automation and Standardization</li>
<li>Research Agenda</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Jack Xia" />
          <p class="site-author-name" itemprop="name">Jack Xia</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">156</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jack Xia</span>
</div>



        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>

<script type="text/javascript" src="//cdn.bootcss.com/mermaid/6.0.0/mermaid.min.js"></script>

  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

</body>
</html>
